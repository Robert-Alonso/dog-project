
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{dog\_app}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Artificial Intelligence
Nanodegree}\label{artificial-intelligence-nanodegree}

\subsection{Convolutional Neural
Networks}\label{convolutional-neural-networks}

\subsection{Project: Write an Algorithm for a Dog Identification
App}\label{project-write-an-algorithm-for-a-dog-identification-app}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this notebook, some template code has already been provided for you,
and you will need to implement additional functionality to successfully
complete this project. You will not need to modify the included code
beyond what is requested. Sections that begin with
\textbf{'(IMPLEMENTATION)'} in the header indicate that the following
block of code will require additional functionality which you must
provide. Instructions will be provided for each section, and the
specifics of the implementation are marked in the code block with a
'TODO' statement. Please be sure to read the instructions carefully!

\begin{quote}
\textbf{Note}: Once you have completed all of the code implementations,
you need to finalize your work by exporting the iPython Notebook as an
HTML document. Before exporting the notebook to html, all of the code
cells need to have been run so that reviewers can see the final
implementation and output. You can then export the notebook by using the
menu above and navigating to \n", "\textbf{File -\textgreater{} Download
as -\textgreater{} HTML (.html)}. Include the finished document along
with this notebook as your submission.
\end{quote}

In addition to implementing code, there will be questions that you must
answer which relate to the project and your implementation. Each section
where you will answer a question is preceded by a \textbf{'Question X'}
header. Carefully read each question and provide thorough answers in the
following text boxes that begin with \textbf{'Answer:'}. Your project
submission will be evaluated based on your answers to each of the
questions and the implementation you provide.

\begin{quote}
\textbf{Note:} Code and Markdown cells can be executed using the
\textbf{Shift + Enter} keyboard shortcut. Markdown cells can be edited
by double-clicking the cell to enter edit mode.
\end{quote}

The rubric contains \emph{optional} "Stand Out Suggestions" for
enhancing the project beyond the minimum requirements. If you decide to
pursue the "Stand Out Suggestions", you should include the code in this
IPython notebook.

 \#\# Step 0: Import Datasets

\subsubsection{Import Dog Dataset}\label{import-dog-dataset}

In the code cell below, we import a dataset of dog images. We populate a
few variables through the use of the \texttt{load\_files} function from
the scikit-learn library: - \texttt{train\_files},
\texttt{valid\_files}, \texttt{test\_files} - numpy arrays containing
file paths to images - \texttt{train\_targets}, \texttt{valid\_targets},
\texttt{test\_targets} - numpy arrays containing onehot-encoded
classification labels - \texttt{dog\_names} - list of string-valued dog
breed names for translating labels

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{load\PYZus{}files}       
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{np\PYZus{}utils}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{glob} \PY{k}{import} \PY{n}{glob}
        
        \PY{c+c1}{\PYZsh{} define function to load train, test, and validation datasets}
        \PY{k}{def} \PY{n+nf}{load\PYZus{}dataset}\PY{p}{(}\PY{n}{path}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{load\PYZus{}files}\PY{p}{(}\PY{n}{path}\PY{p}{)}
            \PY{n}{dog\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{filenames}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{n}{dog\PYZus{}targets} \PY{o}{=} \PY{n}{np\PYZus{}utils}\PY{o}{.}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{133}\PY{p}{)}
            \PY{k}{return} \PY{n}{dog\PYZus{}files}\PY{p}{,} \PY{n}{dog\PYZus{}targets}
        
        \PY{c+c1}{\PYZsh{} load train, test, and validation datasets}
        \PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{train\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} load list of dog names}
        \PY{n}{dog\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{n}{item}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dogImages/train/*/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} print statistics about the dataset}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total dog categories.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}names}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ total dog images.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}files}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ training dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ validation dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ test dog images.}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 133 total dog categories.
There are 8351 total dog images.

There are 6680 training dog images.
There are 835 validation dog images.
There are 836 test dog images.

    \end{Verbatim}

    \subsubsection{Import Human Dataset}\label{import-human-dataset}

In the code cell below, we import a dataset of human images, where the
file paths are stored in the numpy array \texttt{human\_files}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}129}]:} \PY{k+kn}{import} \PY{n+nn}{random}
          \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{8675309}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} load filenames in shuffled human dataset}
          \PY{n}{human\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lfw/*/*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
          \PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} print statistics about the dataset}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total human images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 13233 total human images.

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 1: Detect Humans

We use OpenCV's implementation of
\href{http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html}{Haar
feature-based cascade classifiers} to detect human faces in images.
OpenCV provides many pre-trained face detectors, stored as XML files on
\href{https://github.com/opencv/opencv/tree/master/data/haarcascades}{github}.
We have downloaded one of these detectors and stored it in the
\texttt{haarcascades} directory.

In the next code cell, we demonstrate how to use this detector to find
human faces in a sample image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}141}]:} \PY{k+kn}{import} \PY{n+nn}{cv2}    
          \PY{k+kn}{import} \PY{n+nn}{urllib}
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}                        
          \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline                               
          
          \PY{c+c1}{\PYZsh{} extract pre\PYZhy{}trained face detector}
          \PY{n}{face\PYZus{}cascade} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{CascadeClassifier}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{haarcascades/haarcascade\PYZus{}frontalface\PYZus{}alt.xml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} load color (BGR) image}
          \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{[}\PY{l+m+mi}{459}\PY{p}{]}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} convert BGR image to grayscale}
          \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} find faces in image}
          \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} print number of faces detected in the image}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of faces detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} get bounding box for each detected face}
          \PY{k}{for} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{n}{h}\PY{p}{)} \PY{o+ow}{in} \PY{n}{faces}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} add bounding box to color image}
              \PY{n}{cv2}\PY{o}{.}\PY{n}{rectangle}\PY{p}{(}\PY{n}{img}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{o}{+}\PY{n}{w}\PY{p}{,}\PY{n}{y}\PY{o}{+}\PY{n}{h}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{255}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
              
          \PY{c+c1}{\PYZsh{} convert BGR image to RGB for plotting}
          \PY{n}{cv\PYZus{}rgb} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2RGB}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} display the image, along with bounding box}
          \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cv\PYZus{}rgb}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of faces detected: 1

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Before using any of the face detectors, it is standard procedure to
convert the images to grayscale. The \texttt{detectMultiScale} function
executes the classifier stored in \texttt{face\_cascade} and takes the
grayscale image as a parameter.

In the above code, \texttt{faces} is a numpy array of detected faces,
where each row corresponds to a detected face. Each detected face is a
1D array with four entries that specifies the bounding box of the
detected face. The first two entries in the array (extracted in the
above code as \texttt{x} and \texttt{y}) specify the horizontal and
vertical positions of the top left corner of the bounding box. The last
two entries in the array (extracted here as \texttt{w} and \texttt{h})
specify the width and height of the box.

\subsubsection{Write a Human Face
Detector}\label{write-a-human-face-detector}

We can use this procedure to write a function that returns \texttt{True}
if a human face is detected in an image and \texttt{False} otherwise.
This function, aptly named \texttt{face\_detector}, takes a
string-valued file path to an image as input and appears in the code
block below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}151}]:} \PY{c+c1}{\PYZsh{} returns \PYZdq{}True\PYZdq{} if face is detected in image stored at img\PYZus{}path}
          \PY{k}{def} \PY{n+nf}{face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{k}{if} \PY{n}{url}\PY{p}{:}
                  \PY{n}{req} \PY{o}{=} \PY{n}{urllib}\PY{o}{.}\PY{n}{request}\PY{o}{.}\PY{n}{urlopen}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
                  \PY{n}{arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n+nb}{bytearray}\PY{p}{(}\PY{n}{req}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imdecode}\PY{p}{(}\PY{n}{arr}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
              \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
              \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
              \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Assess the Human Face
Detector}\label{implementation-assess-the-human-face-detector}

\textbf{Question 1:} Use the code cell below to test the performance of
the \texttt{face\_detector} function.\\
- What percentage of the first 100 images in \texttt{human\_files} have
a detected human face?\\
- What percentage of the first 100 images in \texttt{dog\_files} have a
detected human face?

Ideally, we would like 100\% of human images with a detected face and
0\% of dog images with a detected face. You will see that our algorithm
falls short of this goal, but still gives acceptable performance. We
extract the file paths for the first 100 images from each of the
datasets and store them in the numpy arrays \texttt{human\_files\_short}
and \texttt{dog\_files\_short}.

\textbf{Answer:}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{n}{human\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{human\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
         \PY{n}{dog\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} Do NOT modify the code above this line.}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} DONE: Test the performance of the face\PYZus{}detector algorithm }
         \PY{c+c1}{\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         
         \PY{o}{\PYZpc{}}\PY{k}{time} humans\PYZus{}as\PYZus{}humans = sum([int(face\PYZus{}detector(human\PYZus{}file)) for human\PYZus{}file in human\PYZus{}files\PYZus{}short])
         \PY{o}{\PYZpc{}}\PY{k}{time} dogs\PYZus{}as\PYZus{}humans = sum([int(face\PYZus{}detector(dog\PYZus{}file)) for dog\PYZus{}file in dog\PYZus{}files\PYZus{}short])
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}humans\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in human\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}dogs\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in dog\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: user 6.32 s, sys: 0 ns, total: 6.32 s
Wall time: 1.63 s
CPU times: user 34.3 s, sys: 6.68 s, total: 40.9 s
Wall time: 12.1 s
99\% of the first 100 images in human\_files have a detected human face
12\% of the first 100 images in dog\_files have a detected human face

    \end{Verbatim}

    \textbf{Question 2:} This algorithmic choice necessitates that we
communicate to the user that we accept human images only when they
provide a clear view of a face (otherwise, we risk having unneccessarily
frustrated users!). In your opinion, is this a reasonable expectation to
pose on the user? If not, can you think of a way to detect humans in
images that does not necessitate an image with a clearly presented face?

\textbf{Answer:} Considering that the objective of the product is to
output an estimate of a dog breed given a dog or human face, the users
will more probably expect that they need to provide a frontal view of
the face. Furthermore, the Haar cascades are used specifically for face
detection, but not general human detection. In case it is necessary to
detect humans from other kind of angles, a dataset that represents these
cases will be necessary, and although ImageNet does not have a specific
human class, the first layers of a pretrained model could still be
useful to perform transfer learning.

We suggest the face detector from OpenCV as a potential way to detect
human images in your algorithm, but you are free to explore other
approaches, especially approaches that make use of deep learning :).
Please use the code cell below to design and test your own face
detection algorithm. If you decide to pursue this \emph{optional} task,
report performance on each of the datasets.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} (Optional) DONE: Report the performance of another  }
        \PY{c+c1}{\PYZsh{}\PYZsh{} face detection algorithm on the LFW dataset}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
\end{Verbatim}


    \paragraph{LBP Cascades}\label{lbp-cascades}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}153}]:} \PY{n}{lbp\PYZus{}face\PYZus{}cascade} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{CascadeClassifier}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lbpcascades/lbpcascade\PYZus{}frontalface\PYZus{}improved.xml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{k}{def} \PY{n+nf}{lbp\PYZus{}face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{k}{if} \PY{n}{url}\PY{p}{:}
                  \PY{n}{req} \PY{o}{=} \PY{n}{urllib}\PY{o}{.}\PY{n}{request}\PY{o}{.}\PY{n}{urlopen}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
                  \PY{n}{arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n+nb}{bytearray}\PY{p}{(}\PY{n}{req}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imdecode}\PY{p}{(}\PY{n}{arr}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
              \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
              \PY{n}{faces} \PY{o}{=} \PY{n}{lbp\PYZus{}face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
              \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Test the performance of the lbp\PYZus{}face\PYZus{}detector algorithm }
         \PY{c+c1}{\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         
         \PY{o}{\PYZpc{}}\PY{k}{time} humans\PYZus{}as\PYZus{}humans = sum([int(lbp\PYZus{}face\PYZus{}detector(human\PYZus{}file)) for human\PYZus{}file in human\PYZus{}files\PYZus{}short])
         \PY{o}{\PYZpc{}}\PY{k}{time} dogs\PYZus{}as\PYZus{}humans = sum([int(lbp\PYZus{}face\PYZus{}detector(dog\PYZus{}file)) for dog\PYZus{}file in dog\PYZus{}files\PYZus{}short])
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}humans\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in human\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}dogs\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in dog\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: user 1.71 s, sys: 0 ns, total: 1.71 s
Wall time: 522 ms
CPU times: user 14.8 s, sys: 0 ns, total: 14.8 s
Wall time: 2.74 s
87\% of the first 100 images in human\_files have a detected human face
1\% of the first 100 images in dog\_files have a detected human face

    \end{Verbatim}

    \paragraph{Deep Neural Network
(ResNet-10)}\label{deep-neural-network-resnet-10}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}124}]:} \PY{n}{net} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{dnn}\PY{o}{.}\PY{n}{readNetFromCaffe}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{caffe/deploy.prototxt.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                         \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{caffe/res10\PYZus{}300x300\PYZus{}ssd\PYZus{}iter\PYZus{}140000.caffemodel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}154}]:} \PY{k}{def} \PY{n+nf}{dnn\PYZus{}face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{confidence\PYZus{}threshold}\PY{o}{=}\PY{l+m+mf}{0.95}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{k}{if} \PY{n}{url}\PY{p}{:}
                  \PY{n}{req} \PY{o}{=} \PY{n}{urllib}\PY{o}{.}\PY{n}{request}\PY{o}{.}\PY{n}{urlopen}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
                  \PY{n}{arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n+nb}{bytearray}\PY{p}{(}\PY{n}{req}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imdecode}\PY{p}{(}\PY{n}{arr}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
              \PY{n}{h}\PY{p}{,} \PY{n}{w} \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
              \PY{n}{blob} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{dnn}\PY{o}{.}\PY{n}{blobFromImage}\PY{p}{(}\PY{n}{cv2}\PY{o}{.}\PY{n}{resize}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{l+m+mi}{300}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                          \PY{l+m+mf}{1.0}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{300}\PY{p}{,} \PY{l+m+mi}{300}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{104.0}\PY{p}{,} \PY{l+m+mf}{177.0}\PY{p}{,} \PY{l+m+mf}{123.0}\PY{p}{)}\PY{p}{)}
              \PY{n}{net}\PY{o}{.}\PY{n}{setInput}\PY{p}{(}\PY{n}{blob}\PY{p}{)}
              \PY{n}{faces} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{p}{)}
              \PY{k}{return} \PY{n+nb}{any}\PY{p}{(}\PY{p}{[}\PY{n}{faces}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{n}{confidence\PYZus{}threshold} \PYZbs{}
                              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{faces}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Test the performance of a pretrained ResNet\PYZhy{}10 classifier}
         \PY{c+c1}{\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         
         \PY{o}{\PYZpc{}}\PY{k}{time} humans\PYZus{}as\PYZus{}humans = sum([int(dnn\PYZus{}face\PYZus{}detector(human\PYZus{}file, 0.99)) for human\PYZus{}file in human\PYZus{}files\PYZus{}short])
         \PY{o}{\PYZpc{}}\PY{k}{time} dogs\PYZus{}as\PYZus{}humans = sum([int(dnn\PYZus{}face\PYZus{}detector(dog\PYZus{}file, 0.99)) for dog\PYZus{}file in dog\PYZus{}files\PYZus{}short])
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}humans\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in human\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}dogs\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in dog\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: user 8.54 s, sys: 304 ms, total: 8.84 s
Wall time: 2.5 s
CPU times: user 8.51 s, sys: 244 ms, total: 8.75 s
Wall time: 2.46 s
100\% of the first 100 images in human\_files have a detected human face
4\% of the first 100 images in dog\_files have a detected human face

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Test the performance of a pretrained ResNet\PYZhy{}10 classifier}
         \PY{c+c1}{\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         
         \PY{o}{\PYZpc{}}\PY{k}{time} humans\PYZus{}as\PYZus{}humans = sum([int(dnn\PYZus{}face\PYZus{}detector(human\PYZus{}file, 0.5)) for human\PYZus{}file in human\PYZus{}files\PYZus{}short])
         \PY{o}{\PYZpc{}}\PY{k}{time} dogs\PYZus{}as\PYZus{}humans = sum([int(dnn\PYZus{}face\PYZus{}detector(dog\PYZus{}file, 0.5)) for dog\PYZus{}file in dog\PYZus{}files\PYZus{}short])
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}humans\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in human\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}dogs\PYZus{}as\PYZus{}humans\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in dog\PYZus{}files have a detected human face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: user 8.41 s, sys: 284 ms, total: 8.69 s
Wall time: 2.41 s
CPU times: user 8.13 s, sys: 116 ms, total: 8.25 s
Wall time: 2.38 s
100\% of the first 100 images in human\_files have a detected human face
54\% of the first 100 images in dog\_files have a detected human face

    \end{Verbatim}

    \subsubsection{Results of experimenting with multiple face
detectors:}\label{results-of-experimenting-with-multiple-face-detectors}

    \begin{longtable}[]{@{}lcccc@{}}
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright\strut
Method\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering\strut
Prediction time (humans)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering\strut
Prediction time (dogs)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering\strut
Face detected (humans)\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\centering\strut
Face detected (dogs)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
HAAR Cascades\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
1.63 s\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
12.1 s\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
99\%\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
12\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
LBP Cascades\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
522 ms\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
2.74 s\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
87\%\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
1\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
ResNet-10 (confidence \textgreater{} 0.99)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
2.5 s\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
2.46 s\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
100\%\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
4\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
ResNet-10 (confidence \textgreater{} 0.5)\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
2.41 s\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
2.38 s\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
100\%\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\centering\strut
54\%\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    As we can see, the deep neural network (ResNet) was able to identify all
the human faces even at a high confidence threshold (0.99), achieving
high discriminative capacity between human and dog faces, with a similar
prediction time for human and dog images because of the resizing applied
to both kind of images in order to preprocess them as inputs of the
neural net. On the other hand, LBP Cascades showed to be a fast but less
accurate face detector. Finally, HAAR cascades speed showed to be
affected by the type of images used as input and was able to nearly
detect all the human faces. Because of its accuracy and speed, the
ResNet model will be considered the method of choice.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 2: Detect Dogs

In this section, we use a pre-trained
\href{http://ethereon.github.io/netscope/\#/gist/db945b393d40bfa26006}{ResNet-50}
model to detect dogs in images. Our first line of code downloads the
ResNet-50 model, along with weights that have been trained on
\href{http://www.image-net.org/}{ImageNet}, a very large, very popular
dataset used for image classification and other vision tasks. ImageNet
contains over 10 million URLs, each linking to an image containing an
object from one of
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{1000
categories}. Given an image, this pre-trained ResNet-50 model returns a
prediction (derived from the available categories in ImageNet) for the
object that is contained in the image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}131}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{ResNet50}
          
          \PY{c+c1}{\PYZsh{} define ResNet50 model}
          \PY{n}{ResNet50\PYZus{}model} \PY{o}{=} \PY{n}{ResNet50}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}165}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{image}                  
          \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}\PY{p}{,} \PY{n}{tqdm\PYZus{}notebook}
          
          \PY{k}{def} \PY{n+nf}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} loads RGB image as PIL.Image.Image type}
              \PY{n}{img} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{load\PYZus{}img}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{)}\PY{p}{)}
              \PY{c+c1}{\PYZsh{} convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)}
              \PY{n}{x} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{img\PYZus{}to\PYZus{}array}\PY{p}{(}\PY{n}{img}\PY{p}{)}
              \PY{c+c1}{\PYZsh{} convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor}
              \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
          
          \PY{k}{def} \PY{n+nf}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{:}
              \PY{n}{list\PYZus{}of\PYZus{}tensors} \PY{o}{=} \PY{p}{[}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)} \PY{k}{for} \PY{n}{img\PYZus{}path} \PY{o+ow}{in} \PY{n}{tqdm\PYZus{}notebook}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{]}
              \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}162}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{preprocess\PYZus{}input}\PY{p}{,} \PY{n}{decode\PYZus{}predictions}
          
          \PY{k}{def} \PY{n+nf}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} returns prediction vector for image located at img\PYZus{}path}
              \PY{n}{img} \PY{o}{=} \PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{p}{)}\PY{p}{)}
              \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{ResNet50\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}161}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} returns \PYZdq{}True\PYZdq{} if a dog is detected in the image stored at img\PYZus{}path}
          \PY{k}{def} \PY{n+nf}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{n}{prediction} \PY{o}{=} \PY{n}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{p}{)}
              \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{268}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{151}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}


    \subsubsection{Pre-process the Data}\label{pre-process-the-data}

When using TensorFlow as backend, Keras CNNs require a 4D array (which
we'll also refer to as a 4D tensor) as input, with shape

\[
(\text{nb_samples}, \text{rows}, \text{columns}, \text{channels}),
\]

where \texttt{nb\_samples} corresponds to the total number of images (or
samples), and \texttt{rows}, \texttt{columns}, and \texttt{channels}
correspond to the number of rows, columns, and channels for each image,
respectively.

The \texttt{path\_to\_tensor} function below takes a string-valued file
path to a color image as input and returns a 4D tensor suitable for
supplying to a Keras CNN. The function first loads the image and resizes
it to a square image that is \(224 \times 224\) pixels. Next, the image
is converted to an array, which is then resized to a 4D tensor. In this
case, since we are working with color images, each image has three
channels. Likewise, since we are processing a single image (or sample),
the returned tensor will always have shape

\[
(1, 224, 224, 3).
\]

The \texttt{paths\_to\_tensor} function takes a numpy array of
string-valued image paths as input and returns a 4D tensor with shape

\[
(\text{nb_samples}, 224, 224, 3).
\]

Here, \texttt{nb\_samples} is the number of samples, or number of
images, in the supplied array of image paths. It is best to think of
\texttt{nb\_samples} as the number of 3D tensors (where each 3D tensor
corresponds to a different image) in your dataset!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{image}                  
        \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}\PY{p}{,} \PY{n}{tqdm\PYZus{}notebook}
        
        \PY{k}{def} \PY{n+nf}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} loads RGB image as PIL.Image.Image type}
            \PY{n}{img} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{load\PYZus{}img}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)}
            \PY{n}{x} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{img\PYZus{}to\PYZus{}array}\PY{p}{(}\PY{n}{img}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{:}
            \PY{n}{list\PYZus{}of\PYZus{}tensors} \PY{o}{=} \PY{p}{[}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)} \PY{k}{for} \PY{n}{img\PYZus{}path} \PY{o+ow}{in} \PY{n}{tqdm\PYZus{}notebook}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{]}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Making Predictions with
ResNet-50}\label{making-predictions-with-resnet-50}

Getting the 4D tensor ready for ResNet-50, and for any other pre-trained
model in Keras, requires some additional processing. First, the RGB
image is converted to BGR by reordering the channels. All pre-trained
models have the additional normalization step that the mean pixel
(expressed in RGB as \([103.939, 116.779, 123.68]\) and calculated from
all pixels in all images in ImageNet) must be subtracted from every
pixel in each image. This is implemented in the imported function
\texttt{preprocess\_input}. If you're curious, you can check the code
for \texttt{preprocess\_input}
\href{https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py}{here}.

Now that we have a way to format our image for supplying to ResNet-50,
we are now ready to use the model to extract the predictions. This is
accomplished with the \texttt{predict} method, which returns an array
whose \(i\)-th entry is the model's predicted probability that the image
belongs to the \(i\)-th ImageNet category. This is implemented in the
\texttt{ResNet50\_predict\_labels} function below.

By taking the argmax of the predicted probability vector, we obtain an
integer corresponding to the model's predicted object class, which we
can identify with an object category through the use of this
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{preprocess\PYZus{}input}\PY{p}{,} \PY{n}{decode\PYZus{}predictions}
         
         \PY{k}{def} \PY{n+nf}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} returns prediction vector for image located at img\PYZus{}path}
             \PY{n}{img} \PY{o}{=} \PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{ResNet50\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Write a Dog Detector}\label{write-a-dog-detector}

While looking at the
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary},
you will notice that the categories corresponding to dogs appear in an
uninterrupted sequence and correspond to dictionary keys 151-268,
inclusive, to include all categories from
\texttt{\textquotesingle{}Chihuahua\textquotesingle{}} to
\texttt{\textquotesingle{}Mexican\ hairless\textquotesingle{}}. Thus, in
order to check to see if an image is predicted to contain a dog by the
pre-trained ResNet-50 model, we need only check if the
\texttt{ResNet50\_predict\_labels} function above returns a value
between 151 and 268 (inclusive).

We use these ideas to complete the \texttt{dog\_detector} function
below, which returns \texttt{True} if a dog is detected in an image (and
\texttt{False} if not).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} returns \PYZdq{}True\PYZdq{} if a dog is detected in the image stored at img\PYZus{}path}
         \PY{k}{def} \PY{n+nf}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{prediction} \PY{o}{=} \PY{n}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{268}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{151}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Assess the Dog
Detector}\label{implementation-assess-the-dog-detector}

\textbf{Question 3:} Use the code cell below to test the performance of
your \texttt{dog\_detector} function.\\
- What percentage of the images in \texttt{human\_files\_short} have a
detected dog?\\
- What percentage of the images in \texttt{dog\_files\_short} have a
detected dog?

\textbf{Answer:}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Test the performance of the dog\PYZus{}detector function}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
        
        \PY{o}{\PYZpc{}}\PY{k}{time} humans\PYZus{}as\PYZus{}dogs = sum([int(dog\PYZus{}detector(human\PYZus{}file)) for human\PYZus{}file in human\PYZus{}files\PYZus{}short])
        \PY{o}{\PYZpc{}}\PY{k}{time} dogs\PYZus{}as\PYZus{}dogs = sum([int(dog\PYZus{}detector(dog\PYZus{}file)) for dog\PYZus{}file in dog\PYZus{}files\PYZus{}short])
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}humans\PYZus{}as\PYZus{}dogs\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in human\PYZus{}files have a detected dog face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}dogs\PYZus{}as\PYZus{}dogs\PYZcb{}}\PY{l+s+si}{\PYZpc{} o}\PY{l+s+s1}{f the first 100 images in dog\PYZus{}files have a detected dog face}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 3: Create a CNN to Classify Dog Breeds (from Scratch)

Now that we have functions for detecting humans and dogs in images, we
need a way to predict breed from images. In this step, you will create a
CNN that classifies dog breeds. You must create your CNN \emph{from
scratch} (so, you can't use transfer learning \emph{yet}!), and you must
attain a test accuracy of at least 1\%. In Step 5 of this notebook, you
will have the opportunity to use transfer learning to create a CNN that
attains greatly improved accuracy.

Be careful with adding too many trainable layers! More parameters means
longer training, which means you are more likely to need a GPU to
accelerate the training process. Thankfully, Keras provides a handy
estimate of the time that each epoch is likely to take; you can
extrapolate this estimate to figure out how long it will take for your
algorithm to train.

We mention that the task of assigning breed to dogs from images is
considered exceptionally challenging. To see why, consider that
\emph{even a human} would have great difficulty in distinguishing
between a Brittany and a Welsh Springer Spaniel.

\begin{longtable}[]{@{}ll@{}}
\toprule
Brittany & Welsh Springer Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

It is not difficult to find other dog breed pairs with minimal
inter-class variation (for instance, Curly-Coated Retrievers and
American Water Spaniels).

\begin{longtable}[]{@{}ll@{}}
\toprule
Curly-Coated Retriever & American Water Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

Likewise, recall that labradors come in yellow, chocolate, and black.
Your vision-based algorithm will have to conquer this high intra-class
variation to determine how to classify all of these different shades as
the same breed.

\begin{longtable}[]{@{}ll@{}}
\toprule
Yellow Labrador & Chocolate Labrador\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

We also mention that random chance presents an exceptionally low bar:
setting aside the fact that the classes are slightly imabalanced, a
random guess will provide a correct answer roughly 1 in 133 times, which
corresponds to an accuracy of less than 1\%.

Remember that the practice is far ahead of the theory in deep learning.
Experiment with many different architectures, and trust your intuition.
And, of course, have fun!

\subsubsection{Pre-process the Data}\label{pre-process-the-data}

We rescale the images by dividing every pixel in every image by 255.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{ImageFile}                            
         \PY{n}{ImageFile}\PY{o}{.}\PY{n}{LOAD\PYZus{}TRUNCATED\PYZus{}IMAGES} \PY{o}{=} \PY{k+kc}{True}                 
         
         \PY{c+c1}{\PYZsh{} pre\PYZhy{}process the data for Keras}
         \PY{n}{train\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{valid\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{test\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
\end{Verbatim}


    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, max=6680), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, max=835), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, max=836), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]


    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    model.summary()
\end{verbatim}

We have imported some Python modules to get you started, but feel free
to import as many modules as you need. If you end up getting stuck,
here's a hint that specifies a model that trains relatively fast on CPU
and attains \textgreater{}1\% test accuracy in 5 epochs:

\begin{figure}
\centering
\includegraphics{images/sample_cnn.png}
\caption{Sample CNN}
\end{figure}

\textbf{Question 4:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. If you chose to use the
hinted architecture above, describe why you think that CNN architecture
should work well for the image classification task.

\textbf{Answer:} A sequence of 4 convolutional modules were used, each
module consisted of a convolutional layer with a 5x5 kernel and a stride
of 3, with a increasing number of filters (16, 32, 64, 128), under the
intuition that reducing the spatial dimensions through the kernel size
will approximate translation invariance, and increasing the number of
filters will provide space for more higher level features. The module
also included a ReLU activation layer to reduce the likelihood of
vanishing gradients, a Batch Normalization layer for speeding up the
training of the neural network and a Dropout layer (keep\_prob=0.5) as a
regularization technique for preventing overfitting. Max pooling layers
were not considered because of the convolutional layer stride, based on
the work of Springenberg et al.(1) and Howard et al.(2). Finally, the
last layers consist of a 2D Global Average Pooling layer and a couple of
dense layers, in order to find patterns from the extracted image
features.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  2015 - Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox,
  Martin Riedmiller. Striving for Simplicity: The All Convolutional Net
\item
  2017 - Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
  Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. MobileNets:
  Efficient Convolutional Neural Networks for Mobile Vision Applications
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv2D}\PY{p}{,} \PY{n}{MaxPooling2D}\PY{p}{,} \PY{n}{GlobalAveragePooling2D}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dense}\PY{p}{,} \PY{n}{Activation}\PY{p}{,} \PY{n}{BatchNormalization}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Define your architecture.}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{128}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv2d\_9 (Conv2D)            (None, 220, 220, 16)      1216      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_9 (Activation)    (None, 220, 220, 16)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_9 (Batch (None, 220, 220, 16)      64        
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_9 (Dropout)          (None, 220, 220, 16)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_10 (Conv2D)           (None, 72, 72, 32)        12832     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_10 (Activation)   (None, 72, 72, 32)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_10 (Batc (None, 72, 72, 32)        128       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_10 (Dropout)         (None, 72, 72, 32)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_11 (Conv2D)           (None, 23, 23, 64)        51264     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_11 (Activation)   (None, 23, 23, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_11 (Batc (None, 23, 23, 64)        256       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_11 (Dropout)         (None, 23, 23, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_12 (Conv2D)           (None, 7, 7, 128)         204928    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_12 (Activation)   (None, 7, 7, 128)         0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_12 (Batc (None, 7, 7, 128)         512       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_12 (Dropout)         (None, 7, 7, 128)         0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling2d\_3 ( (None, 128)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_3 (Dense)              (None, 512)               66048     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_13 (Dropout)         (None, 512)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_4 (Dense)              (None, 133)               68229     
=================================================================
Total params: 405,477
Trainable params: 404,997
Non-trainable params: 480
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{Compile the Model}\label{compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}  
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: specify the number of epochs that you would like to use to train the model.}
         
         \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{40}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Do NOT modify the code below this line.}
         
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}tensors}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                       \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}tensors}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                       \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.9864 - acc: 0.2486
Epoch 00001: val\_loss improved from inf to 6.26468, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 15s 2ms/step - loss: 2.9873 - acc: 0.2487 - val\_loss: 6.2647 - val\_acc: 0.0994
Epoch 2/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.9188 - acc: 0.2640
Epoch 00002: val\_loss improved from 6.26468 to 3.82390, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 14s 2ms/step - loss: 2.9163 - acc: 0.2644 - val\_loss: 3.8239 - val\_acc: 0.1509
Epoch 3/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.8708 - acc: 0.2679
Epoch 00003: val\_loss improved from 3.82390 to 3.62868, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 14s 2ms/step - loss: 2.8723 - acc: 0.2671 - val\_loss: 3.6287 - val\_acc: 0.1665
Epoch 4/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.8723 - acc: 0.2649
Epoch 00004: val\_loss did not improve
6680/6680 [==============================] - 13s 2ms/step - loss: 2.8726 - acc: 0.2651 - val\_loss: 3.7756 - val\_acc: 0.1796
Epoch 5/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.8311 - acc: 0.2829
Epoch 00005: val\_loss improved from 3.62868 to 3.59471, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 14s 2ms/step - loss: 2.8311 - acc: 0.2828 - val\_loss: 3.5947 - val\_acc: 0.1796
Epoch 6/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.7939 - acc: 0.2850
Epoch 00006: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.7942 - acc: 0.2852 - val\_loss: 3.6741 - val\_acc: 0.1605
Epoch 7/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.7797 - acc: 0.2897
Epoch 00007: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.7817 - acc: 0.2897 - val\_loss: 3.6670 - val\_acc: 0.1641
Epoch 8/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.7375 - acc: 0.3005
Epoch 00008: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.7387 - acc: 0.3000 - val\_loss: 3.8703 - val\_acc: 0.1533
Epoch 9/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.7370 - acc: 0.2996
Epoch 00009: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.7383 - acc: 0.2990 - val\_loss: 4.6702 - val\_acc: 0.0934
Epoch 10/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.7203 - acc: 0.2975
Epoch 00010: val\_loss improved from 3.59471 to 3.45169, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 14s 2ms/step - loss: 2.7210 - acc: 0.2972 - val\_loss: 3.4517 - val\_acc: 0.1976
Epoch 11/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.6862 - acc: 0.3057
Epoch 00011: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.6876 - acc: 0.3057 - val\_loss: 3.7094 - val\_acc: 0.1665
Epoch 12/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.6745 - acc: 0.3108
Epoch 00012: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.6750 - acc: 0.3106 - val\_loss: 3.7665 - val\_acc: 0.1341
Epoch 13/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.6281 - acc: 0.3139
Epoch 00013: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.6279 - acc: 0.3138 - val\_loss: 3.5918 - val\_acc: 0.1796
Epoch 14/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.6500 - acc: 0.3137
Epoch 00014: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.6538 - acc: 0.3132 - val\_loss: 3.6643 - val\_acc: 0.1892
Epoch 15/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.6086 - acc: 0.3175
Epoch 00015: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.6108 - acc: 0.3169 - val\_loss: 3.7485 - val\_acc: 0.1820
Epoch 16/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5982 - acc: 0.3248
Epoch 00016: val\_loss improved from 3.45169 to 3.43089, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 14s 2ms/step - loss: 2.5969 - acc: 0.3254 - val\_loss: 3.4309 - val\_acc: 0.2180
Epoch 17/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5750 - acc: 0.3313
Epoch 00017: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.5769 - acc: 0.3308 - val\_loss: 4.6636 - val\_acc: 0.1234
Epoch 18/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5783 - acc: 0.3250
Epoch 00018: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.5786 - acc: 0.3247 - val\_loss: 3.5353 - val\_acc: 0.1916
Epoch 19/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5572 - acc: 0.3334
Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.5587 - acc: 0.3331 - val\_loss: 4.0068 - val\_acc: 0.1305
Epoch 20/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5304 - acc: 0.3394
Epoch 00020: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.5316 - acc: 0.3389 - val\_loss: 3.9195 - val\_acc: 0.1760
Epoch 21/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.4989 - acc: 0.3450
Epoch 00021: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.5019 - acc: 0.3443 - val\_loss: 3.7685 - val\_acc: 0.1473
Epoch 22/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5155 - acc: 0.3358
Epoch 00022: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.5182 - acc: 0.3353 - val\_loss: 3.5758 - val\_acc: 0.1808
Epoch 23/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.4723 - acc: 0.3498
Epoch 00023: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.4713 - acc: 0.3500 - val\_loss: 3.5254 - val\_acc: 0.1916
Epoch 24/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.4729 - acc: 0.3472
Epoch 00024: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.4746 - acc: 0.3470 - val\_loss: 3.6247 - val\_acc: 0.2000
Epoch 25/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.4661 - acc: 0.3428
Epoch 00025: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.4654 - acc: 0.3433 - val\_loss: 3.8054 - val\_acc: 0.1749
Epoch 26/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.4521 - acc: 0.3472
Epoch 00026: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.4531 - acc: 0.3469 - val\_loss: 4.1963 - val\_acc: 0.1425
Epoch 27/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.4409 - acc: 0.3612
Epoch 00027: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.4434 - acc: 0.3609 - val\_loss: 4.2925 - val\_acc: 0.1377
Epoch 28/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.4125 - acc: 0.3604
Epoch 00028: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.4125 - acc: 0.3608 - val\_loss: 3.9984 - val\_acc: 0.1701
Epoch 29/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.3804 - acc: 0.3708
Epoch 00029: val\_loss did not improve
6680/6680 [==============================] - 14s 2ms/step - loss: 2.3824 - acc: 0.3705 - val\_loss: 3.9318 - val\_acc: 0.1892
Epoch 30/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.3647 - acc: 0.3708
Epoch 00030: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.3672 - acc: 0.3705 - val\_loss: 3.7816 - val\_acc: 0.1868
Epoch 31/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.3543 - acc: 0.3723
Epoch 00031: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.3550 - acc: 0.3722 - val\_loss: 3.6207 - val\_acc: 0.1928
Epoch 32/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.3339 - acc: 0.3758
Epoch 00032: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.3352 - acc: 0.3753 - val\_loss: 4.2900 - val\_acc: 0.1749
Epoch 33/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.3527 - acc: 0.3756
Epoch 00033: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.3529 - acc: 0.3756 - val\_loss: 3.6435 - val\_acc: 0.1856
Epoch 34/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.3494 - acc: 0.3794
Epoch 00034: val\_loss improved from 3.43089 to 3.38973, saving model to saved\_models/weights.best.from\_scratch.hdf5
6680/6680 [==============================] - 15s 2ms/step - loss: 2.3503 - acc: 0.3792 - val\_loss: 3.3897 - val\_acc: 0.1772
Epoch 35/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.3269 - acc: 0.3779
Epoch 00035: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.3267 - acc: 0.3783 - val\_loss: 3.4723 - val\_acc: 0.2156
Epoch 36/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.2913 - acc: 0.3890
Epoch 00036: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.2955 - acc: 0.3888 - val\_loss: 3.9488 - val\_acc: 0.1796
Epoch 37/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.2885 - acc: 0.3858
Epoch 00037: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.2893 - acc: 0.3856 - val\_loss: 3.6580 - val\_acc: 0.2012
Epoch 38/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.2940 - acc: 0.3863
Epoch 00038: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.2949 - acc: 0.3858 - val\_loss: 3.7836 - val\_acc: 0.1760
Epoch 39/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.2904 - acc: 0.3849
Epoch 00039: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.2928 - acc: 0.3846 - val\_loss: 3.9557 - val\_acc: 0.1796
Epoch 40/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.2584 - acc: 0.3941
Epoch 00040: val\_loss did not improve
6680/6680 [==============================] - 15s 2ms/step - loss: 2.2600 - acc: 0.3934 - val\_loss: 5.2111 - val\_acc: 0.1030

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
         \PY{o}{\PYZpc{}}\PY{k}{config} InlineBackend.figure\PYZus{}format = \PYZsq{}retina\PYZsq{}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
         \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{plot\PYZus{}losses}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{val\PYZus{}loss}\PY{p}{,} \PY{n}{scale}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{n}{x} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{n}{scale} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{val\PYZus{}loss}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{plot\PYZus{}losses}\PY{p}{(}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Test the Model}\label{test-the-model}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 1\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{dog\PYZus{}breed\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{tensor} \PY{o+ow}{in} \PY{n}{test\PYZus{}tensors}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 19.9761\%

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 4: Use a CNN to Classify Dog Breeds

To reduce training time without sacrificing accuracy, we show you how to
train a CNN using transfer learning. In the following step, you will get
a chance to use transfer learning to train your own CNN.

\subsubsection{Obtain Bottleneck
Features}\label{obtain-bottleneck-features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/DogVGG16Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{valid\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{test\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \subsubsection{Model Architecture}\label{model-architecture}

The model uses the the pre-trained VGG-16 model as a fixed feature
extractor, where the last convolutional output of VGG-16 is fed as input
to our model. We only add a global average pooling layer and a fully
connected layer, where the latter contains one node for each dog
category and is equipped with a softmax.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{VGG16\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train\PYZus{}VGG16}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_3 ( (None, 512)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_3 (Dense)              (None, 133)               68229     
=================================================================
Total params: 68,229
Trainable params: 68,229
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{Compile the Model}\label{compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Train the Model}\label{train-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}VGG16}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}VGG16}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/40
6600/6680 [============================>.] - ETA: 0s - loss: 5.8393 - acc: 0.6259
Epoch 00001: val\_loss improved from inf to 6.88083, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 169us/step - loss: 5.8272 - acc: 0.6263 - val\_loss: 6.8808 - val\_acc: 0.4754
Epoch 2/40
6380/6680 [===========================>..] - ETA: 0s - loss: 5.7336 - acc: 0.6353
Epoch 00002: val\_loss did not improve
6680/6680 [==============================] - 1s 139us/step - loss: 5.7986 - acc: 0.6316 - val\_loss: 6.9386 - val\_acc: 0.4766
Epoch 3/40
6500/6680 [============================>.] - ETA: 0s - loss: 5.8238 - acc: 0.6322
Epoch 00003: val\_loss improved from 6.88083 to 6.87993, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 155us/step - loss: 5.7837 - acc: 0.6343 - val\_loss: 6.8799 - val\_acc: 0.4898
Epoch 4/40
6600/6680 [============================>.] - ETA: 0s - loss: 5.7733 - acc: 0.6362
Epoch 00004: val\_loss improved from 6.87993 to 6.87039, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 152us/step - loss: 5.7727 - acc: 0.6362 - val\_loss: 6.8704 - val\_acc: 0.4826
Epoch 5/40
6400/6680 [===========================>..] - ETA: 0s - loss: 5.7670 - acc: 0.6381
Epoch 00005: val\_loss improved from 6.87039 to 6.79696, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 147us/step - loss: 5.7676 - acc: 0.6377 - val\_loss: 6.7970 - val\_acc: 0.4910
Epoch 6/40
6460/6680 [============================>.] - ETA: 0s - loss: 5.7641 - acc: 0.6376
Epoch 00006: val\_loss did not improve
6680/6680 [==============================] - 1s 137us/step - loss: 5.7467 - acc: 0.6386 - val\_loss: 6.8120 - val\_acc: 0.4886
Epoch 7/40
6520/6680 [============================>.] - ETA: 0s - loss: 5.6239 - acc: 0.6388
Epoch 00007: val\_loss did not improve
6680/6680 [==============================] - 1s 135us/step - loss: 5.6098 - acc: 0.6395 - val\_loss: 6.8052 - val\_acc: 0.4886
Epoch 8/40
6640/6680 [============================>.] - ETA: 0s - loss: 5.4934 - acc: 0.6459
Epoch 00008: val\_loss improved from 6.79696 to 6.59751, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 150us/step - loss: 5.4924 - acc: 0.6460 - val\_loss: 6.5975 - val\_acc: 0.5078
Epoch 9/40
6500/6680 [============================>.] - ETA: 0s - loss: 5.3686 - acc: 0.6523
Epoch 00009: val\_loss improved from 6.59751 to 6.50842, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 149us/step - loss: 5.3755 - acc: 0.6518 - val\_loss: 6.5084 - val\_acc: 0.5174
Epoch 10/40
6320/6680 [===========================>..] - ETA: 0s - loss: 5.2391 - acc: 0.6644
Epoch 00010: val\_loss improved from 6.50842 to 6.47848, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 149us/step - loss: 5.2509 - acc: 0.6635 - val\_loss: 6.4785 - val\_acc: 0.5102
Epoch 11/40
6520/6680 [============================>.] - ETA: 0s - loss: 5.0786 - acc: 0.6693
Epoch 00011: val\_loss improved from 6.47848 to 6.26894, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 144us/step - loss: 5.0801 - acc: 0.6696 - val\_loss: 6.2689 - val\_acc: 0.5246
Epoch 12/40
6380/6680 [===========================>..] - ETA: 0s - loss: 4.9034 - acc: 0.6850
Epoch 00012: val\_loss improved from 6.26894 to 6.21898, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 140us/step - loss: 4.9137 - acc: 0.6838 - val\_loss: 6.2190 - val\_acc: 0.5329
Epoch 13/40
6420/6680 [===========================>..] - ETA: 0s - loss: 4.8699 - acc: 0.6879
Epoch 00013: val\_loss improved from 6.21898 to 6.11789, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 152us/step - loss: 4.8538 - acc: 0.6889 - val\_loss: 6.1179 - val\_acc: 0.5293
Epoch 14/40
6260/6680 [===========================>..] - ETA: 0s - loss: 4.7231 - acc: 0.6946
Epoch 00014: val\_loss improved from 6.11789 to 6.11238, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 143us/step - loss: 4.7549 - acc: 0.6924 - val\_loss: 6.1124 - val\_acc: 0.5293
Epoch 15/40
6360/6680 [===========================>..] - ETA: 0s - loss: 4.6234 - acc: 0.6995
Epoch 00015: val\_loss improved from 6.11238 to 6.02042, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 152us/step - loss: 4.6529 - acc: 0.6981 - val\_loss: 6.0204 - val\_acc: 0.5353
Epoch 16/40
6360/6680 [===========================>..] - ETA: 0s - loss: 4.5532 - acc: 0.7071
Epoch 00016: val\_loss improved from 6.02042 to 5.96772, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 149us/step - loss: 4.5606 - acc: 0.7067 - val\_loss: 5.9677 - val\_acc: 0.5401
Epoch 17/40
6620/6680 [============================>.] - ETA: 0s - loss: 4.5334 - acc: 0.7104
Epoch 00017: val\_loss improved from 5.96772 to 5.86728, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 153us/step - loss: 4.5318 - acc: 0.7105 - val\_loss: 5.8673 - val\_acc: 0.5413
Epoch 18/40
6580/6680 [============================>.] - ETA: 0s - loss: 4.5201 - acc: 0.7132
Epoch 00018: val\_loss improved from 5.86728 to 5.82318, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 137us/step - loss: 4.5104 - acc: 0.7139 - val\_loss: 5.8232 - val\_acc: 0.5485
Epoch 19/40
6540/6680 [============================>.] - ETA: 0s - loss: 4.4712 - acc: 0.7150
Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 1s 135us/step - loss: 4.4721 - acc: 0.7150 - val\_loss: 5.8773 - val\_acc: 0.5509
Epoch 20/40
6500/6680 [============================>.] - ETA: 0s - loss: 4.4015 - acc: 0.7189
Epoch 00020: val\_loss did not improve
6680/6680 [==============================] - 1s 138us/step - loss: 4.4093 - acc: 0.7186 - val\_loss: 5.9099 - val\_acc: 0.5425
Epoch 21/40
6380/6680 [===========================>..] - ETA: 0s - loss: 4.4012 - acc: 0.7213
Epoch 00021: val\_loss did not improve
6680/6680 [==============================] - 1s 154us/step - loss: 4.3906 - acc: 0.7219 - val\_loss: 5.8465 - val\_acc: 0.5413
Epoch 22/40
6620/6680 [============================>.] - ETA: 0s - loss: 4.3821 - acc: 0.7230
Epoch 00022: val\_loss improved from 5.82318 to 5.80882, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 136us/step - loss: 4.3814 - acc: 0.7231 - val\_loss: 5.8088 - val\_acc: 0.5473
Epoch 23/40
6640/6680 [============================>.] - ETA: 0s - loss: 4.3725 - acc: 0.7245
Epoch 00023: val\_loss improved from 5.80882 to 5.75495, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 143us/step - loss: 4.3680 - acc: 0.7249 - val\_loss: 5.7550 - val\_acc: 0.5581
Epoch 24/40
6600/6680 [============================>.] - ETA: 0s - loss: 4.3557 - acc: 0.7250
Epoch 00024: val\_loss did not improve
6680/6680 [==============================] - 1s 135us/step - loss: 4.3512 - acc: 0.7253 - val\_loss: 5.9307 - val\_acc: 0.5401
Epoch 25/40
6540/6680 [============================>.] - ETA: 0s - loss: 4.1968 - acc: 0.7252
Epoch 00025: val\_loss improved from 5.75495 to 5.75312, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 145us/step - loss: 4.1876 - acc: 0.7260 - val\_loss: 5.7531 - val\_acc: 0.5485
Epoch 26/40
6420/6680 [===========================>..] - ETA: 0s - loss: 4.0565 - acc: 0.7374
Epoch 00026: val\_loss improved from 5.75312 to 5.68444, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 159us/step - loss: 4.0351 - acc: 0.7388 - val\_loss: 5.6844 - val\_acc: 0.5437
Epoch 27/40
6640/6680 [============================>.] - ETA: 0s - loss: 3.9839 - acc: 0.7453
Epoch 00027: val\_loss improved from 5.68444 to 5.51838, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 150us/step - loss: 3.9817 - acc: 0.7455 - val\_loss: 5.5184 - val\_acc: 0.5689
Epoch 28/40
6300/6680 [===========================>..] - ETA: 0s - loss: 3.9425 - acc: 0.7508
Epoch 00028: val\_loss improved from 5.51838 to 5.48190, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 135us/step - loss: 3.9600 - acc: 0.7494 - val\_loss: 5.4819 - val\_acc: 0.5749
Epoch 29/40
6420/6680 [===========================>..] - ETA: 0s - loss: 3.9174 - acc: 0.7505
Epoch 00029: val\_loss did not improve
6680/6680 [==============================] - 1s 138us/step - loss: 3.9131 - acc: 0.7509 - val\_loss: 5.5036 - val\_acc: 0.5665
Epoch 30/40
6440/6680 [===========================>..] - ETA: 0s - loss: 3.8277 - acc: 0.7542
Epoch 00030: val\_loss did not improve
6680/6680 [==============================] - 1s 138us/step - loss: 3.8079 - acc: 0.7551 - val\_loss: 5.5692 - val\_acc: 0.5689
Epoch 31/40
6360/6680 [===========================>..] - ETA: 0s - loss: 3.7096 - acc: 0.7629
Epoch 00031: val\_loss improved from 5.48190 to 5.37698, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 140us/step - loss: 3.6825 - acc: 0.7648 - val\_loss: 5.3770 - val\_acc: 0.5892
Epoch 32/40
6460/6680 [============================>.] - ETA: 0s - loss: 3.6717 - acc: 0.7684
Epoch 00032: val\_loss improved from 5.37698 to 5.34462, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 140us/step - loss: 3.6597 - acc: 0.7692 - val\_loss: 5.3446 - val\_acc: 0.5856
Epoch 33/40
6520/6680 [============================>.] - ETA: 0s - loss: 3.6571 - acc: 0.7707
Epoch 00033: val\_loss improved from 5.34462 to 5.32195, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 147us/step - loss: 3.6516 - acc: 0.7711 - val\_loss: 5.3219 - val\_acc: 0.5868
Epoch 34/40
6460/6680 [============================>.] - ETA: 0s - loss: 3.6457 - acc: 0.7718
Epoch 00034: val\_loss improved from 5.32195 to 5.31410, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 131us/step - loss: 3.6487 - acc: 0.7717 - val\_loss: 5.3141 - val\_acc: 0.5844
Epoch 35/40
6380/6680 [===========================>..] - ETA: 0s - loss: 3.6603 - acc: 0.7708
Epoch 00035: val\_loss improved from 5.31410 to 5.24148, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s 134us/step - loss: 3.6489 - acc: 0.7716 - val\_loss: 5.2415 - val\_acc: 0.6036
Epoch 36/40
6660/6680 [============================>.] - ETA: 0s - loss: 3.6467 - acc: 0.7725
Epoch 00036: val\_loss did not improve
6680/6680 [==============================] - 1s 148us/step - loss: 3.6453 - acc: 0.7726 - val\_loss: 5.2954 - val\_acc: 0.6024
Epoch 37/40
6380/6680 [===========================>..] - ETA: 0s - loss: 3.6233 - acc: 0.7743
Epoch 00037: val\_loss did not improve
6680/6680 [==============================] - 1s 156us/step - loss: 3.6440 - acc: 0.7731 - val\_loss: 5.2988 - val\_acc: 0.5928
Epoch 38/40
6520/6680 [============================>.] - ETA: 0s - loss: 3.6176 - acc: 0.7753
Epoch 00038: val\_loss did not improve
6680/6680 [==============================] - 1s 144us/step - loss: 3.6430 - acc: 0.7737 - val\_loss: 5.2616 - val\_acc: 0.6036
Epoch 39/40
6320/6680 [===========================>..] - ETA: 0s - loss: 3.6371 - acc: 0.7736
Epoch 00039: val\_loss did not improve
6680/6680 [==============================] - 1s 150us/step - loss: 3.6399 - acc: 0.7734 - val\_loss: 5.3402 - val\_acc: 0.5880
Epoch 40/40
6440/6680 [===========================>..] - ETA: 0s - loss: 3.6435 - acc: 0.7733
Epoch 00040: val\_loss did not improve
6680/6680 [==============================] - 1s 151us/step - loss: 3.6402 - acc: 0.7735 - val\_loss: 5.3392 - val\_acc: 0.5964

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} <keras.callbacks.History at 0x7f6999efbcf8>
\end{Verbatim}
            
    \subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Test the Model}\label{test-the-model}

Now, we can use the CNN to test how well it identifies breed within our
test dataset of dog images. We print the test accuracy below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{VGG16\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}VGG16}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 61.3636\%

    \end{Verbatim}

    \subsubsection{Predict Dog Breed with the
Model}\label{predict-dog-breed-with-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{o}{*}
         
         \PY{k}{def} \PY{n+nf}{VGG16\PYZus{}predict\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} extract bottleneck features}
             \PY{n}{bottleneck\PYZus{}feature} \PY{o}{=} \PY{n}{extract\PYZus{}VGG16}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} obtain predicted vector}
             \PY{n}{predicted\PYZus{}vector} \PY{o}{=} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}feature}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} return dog breed that is predicted by the model}
             \PY{k}{return} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predicted\PYZus{}vector}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 5: Create a CNN to Classify Dog Breeds (using Transfer
Learning)

You will now use transfer learning to create a CNN that can identify dog
breed from images. Your CNN must attain at least 60\% accuracy on the
test set.

In Step 4, we used transfer learning to create a CNN using VGG-16
bottleneck features. In this section, you must use the bottleneck
features from a different pre-trained model. To make things easier for
you, we have pre-computed the features for all of the networks that are
currently available in Keras: -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz}{VGG-19}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz}{ResNet-50}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz}{Inception}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz}{Xception}
bottleneck features

The files are encoded as such:

\begin{verbatim}
Dog{network}Data.npz
\end{verbatim}

where \texttt{\{network\}}, in the above filename, can be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}. Pick one of the above architectures, download the
corresponding bottleneck features, and store the downloaded file in the
\texttt{bottleneck\_features/} folder in the repository.

\subsubsection{(IMPLEMENTATION) Obtain Bottleneck
Features}\label{implementation-obtain-bottleneck-features}

In the code block below, extract the bottleneck features corresponding
to the train, test, and validation sets by running the following:

\begin{verbatim}
bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')
train_{network} = bottleneck_features['train']
valid_{network} = bottleneck_features['valid']
test_{network} = bottleneck_features['test']
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Obtain bottleneck features from another pre\PYZhy{}trained CNN.}
         \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/DogResnet50Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}Resnet50} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{valid\PYZus{}Resnet50} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{test\PYZus{}Resnet50} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    <your model's name>.summary()
\end{verbatim}

\textbf{Question 5:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. Describe why you think the
architecture is suitable for the current problem.

\textbf{Answer:} The architecture was based on the top layers of the
previous CNN (Question 4). As such it consists mainly of dense layers,
in order to analyze the interactions between all the high level
features, and dropout layers, in order to prevent overfitting. As these
are the last layers, it is not necessary to make the CNN very deep,
which was verified by increasing the number of layers and experiencing
an actual drop in performance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Define your architecture.}
         \PY{n}{Resnet50\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train\PYZus{}Resnet50}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_15  (None, 2048)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_34 (Dropout)         (None, 2048)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_27 (Dense)             (None, 512)               1049088   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_35 (Dropout)         (None, 512)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_28 (Dense)             (None, 133)               68229     
=================================================================
Total params: 1,117,317
Trainable params: 1,117,317
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Compile the
Model}\label{implementation-compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Compile the model.}
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Train the model.}
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.Resnet50.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}Resnet50}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}Resnet50}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/40
6656/6680 [============================>.] - ETA: 0s - loss: 3.5455 - acc: 0.2511
Epoch 00001: val\_loss improved from inf to 1.43197, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 2s 227us/step - loss: 3.5406 - acc: 0.2518 - val\_loss: 1.4320 - val\_acc: 0.6240
Epoch 2/40
3584/6680 [===============>{\ldots}] - ETA: 0s - loss: 1.5234 - acc: 0.5776
Epoch 00002: val\_loss improved from 1.43197 to 0.88642, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 22us/step - loss: 1.3446 - acc: 0.6186 - val\_loss: 0.8864 - val\_acc: 0.7437
Epoch 3/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.8840 - acc: 0.7399
Epoch 00003: val\_loss improved from 0.88642 to 0.71095, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 22us/step - loss: 0.8844 - acc: 0.7395 - val\_loss: 0.7110 - val\_acc: 0.7737
Epoch 4/40
3584/6680 [===============>{\ldots}] - ETA: 0s - loss: 0.6988 - acc: 0.7879
Epoch 00004: val\_loss improved from 0.71095 to 0.66366, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 20us/step - loss: 0.6909 - acc: 0.7928 - val\_loss: 0.6637 - val\_acc: 0.7856
Epoch 5/40
4864/6680 [====================>{\ldots}] - ETA: 0s - loss: 0.5481 - acc: 0.8275
Epoch 00005: val\_loss improved from 0.66366 to 0.61468, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 30us/step - loss: 0.5517 - acc: 0.8251 - val\_loss: 0.6147 - val\_acc: 0.8072
Epoch 6/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.4428 - acc: 0.8578
Epoch 00006: val\_loss improved from 0.61468 to 0.59360, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 24us/step - loss: 0.4498 - acc: 0.8572 - val\_loss: 0.5936 - val\_acc: 0.8012
Epoch 7/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.4078 - acc: 0.8745
Epoch 00007: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.3987 - acc: 0.8765 - val\_loss: 0.5944 - val\_acc: 0.7988
Epoch 8/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.3475 - acc: 0.8842
Epoch 00008: val\_loss improved from 0.59360 to 0.56065, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 16us/step - loss: 0.3378 - acc: 0.8882 - val\_loss: 0.5606 - val\_acc: 0.8084
Epoch 9/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.3196 - acc: 0.8965
Epoch 00009: val\_loss improved from 0.56065 to 0.54577, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 17us/step - loss: 0.3133 - acc: 0.8969 - val\_loss: 0.5458 - val\_acc: 0.8216
Epoch 10/40
4096/6680 [=================>{\ldots}] - ETA: 0s - loss: 0.2735 - acc: 0.9141
Epoch 00010: val\_loss did not improve
6680/6680 [==============================] - 0s 15us/step - loss: 0.2722 - acc: 0.9135 - val\_loss: 0.5515 - val\_acc: 0.8263
Epoch 11/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.2524 - acc: 0.9223
Epoch 00011: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.2494 - acc: 0.9238 - val\_loss: 0.5513 - val\_acc: 0.8240
Epoch 12/40
4096/6680 [=================>{\ldots}] - ETA: 0s - loss: 0.2086 - acc: 0.9375
Epoch 00012: val\_loss did not improve
6680/6680 [==============================] - 0s 16us/step - loss: 0.2147 - acc: 0.9338 - val\_loss: 0.5546 - val\_acc: 0.8192
Epoch 13/40
4096/6680 [=================>{\ldots}] - ETA: 0s - loss: 0.2262 - acc: 0.9309
Epoch 00013: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.2233 - acc: 0.9320 - val\_loss: 0.5712 - val\_acc: 0.8144
Epoch 14/40
4096/6680 [=================>{\ldots}] - ETA: 0s - loss: 0.1972 - acc: 0.9380
Epoch 00014: val\_loss improved from 0.54577 to 0.52995, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 21us/step - loss: 0.1901 - acc: 0.9389 - val\_loss: 0.5299 - val\_acc: 0.8335
Epoch 15/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.1843 - acc: 0.9440
Epoch 00015: val\_loss did not improve
6680/6680 [==============================] - 0s 23us/step - loss: 0.1874 - acc: 0.9431 - val\_loss: 0.5452 - val\_acc: 0.8383
Epoch 16/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.1624 - acc: 0.9490
Epoch 00016: val\_loss did not improve
6680/6680 [==============================] - 0s 17us/step - loss: 0.1583 - acc: 0.9493 - val\_loss: 0.5343 - val\_acc: 0.8347
Epoch 17/40
6400/6680 [===========================>..] - ETA: 0s - loss: 0.1530 - acc: 0.9558
Epoch 00017: val\_loss did not improve
6680/6680 [==============================] - 0s 19us/step - loss: 0.1548 - acc: 0.9543 - val\_loss: 0.5644 - val\_acc: 0.8240
Epoch 18/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.1565 - acc: 0.9504
Epoch 00018: val\_loss did not improve
6680/6680 [==============================] - 0s 13us/step - loss: 0.1534 - acc: 0.9534 - val\_loss: 0.5389 - val\_acc: 0.8395
Epoch 19/40
4864/6680 [====================>{\ldots}] - ETA: 0s - loss: 0.1282 - acc: 0.9618
Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 0s 13us/step - loss: 0.1211 - acc: 0.9645 - val\_loss: 0.5394 - val\_acc: 0.8240
Epoch 20/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.1158 - acc: 0.9648
Epoch 00020: val\_loss did not improve
6680/6680 [==============================] - 0s 13us/step - loss: 0.1205 - acc: 0.9632 - val\_loss: 0.5364 - val\_acc: 0.8407
Epoch 21/40
3840/6680 [================>{\ldots}] - ETA: 0s - loss: 0.1404 - acc: 0.9578
Epoch 00021: val\_loss did not improve
6680/6680 [==============================] - 0s 16us/step - loss: 0.1379 - acc: 0.9587 - val\_loss: 0.5541 - val\_acc: 0.8216
Epoch 22/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.1375 - acc: 0.9583
Epoch 00022: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.1348 - acc: 0.9591 - val\_loss: 0.5353 - val\_acc: 0.8287
Epoch 23/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.1059 - acc: 0.9704
Epoch 00023: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.1078 - acc: 0.9681 - val\_loss: 0.5390 - val\_acc: 0.8431
Epoch 24/40
3840/6680 [================>{\ldots}] - ETA: 0s - loss: 0.1052 - acc: 0.9680
Epoch 00024: val\_loss did not improve
6680/6680 [==============================] - 0s 15us/step - loss: 0.1073 - acc: 0.9674 - val\_loss: 0.5531 - val\_acc: 0.8251
Epoch 25/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.0959 - acc: 0.9707
Epoch 00025: val\_loss did not improve
6680/6680 [==============================] - 0s 13us/step - loss: 0.0947 - acc: 0.9708 - val\_loss: 0.5309 - val\_acc: 0.8311
Epoch 26/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.0925 - acc: 0.9729
Epoch 00026: val\_loss did not improve
6680/6680 [==============================] - 0s 13us/step - loss: 0.0926 - acc: 0.9732 - val\_loss: 0.5555 - val\_acc: 0.8407
Epoch 27/40
5376/6680 [=======================>{\ldots}] - ETA: 0s - loss: 0.0836 - acc: 0.9749
Epoch 00027: val\_loss did not improve
6680/6680 [==============================] - 0s 21us/step - loss: 0.0874 - acc: 0.9734 - val\_loss: 0.5346 - val\_acc: 0.8347
Epoch 28/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.0926 - acc: 0.9706
Epoch 00028: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.0894 - acc: 0.9717 - val\_loss: 0.5501 - val\_acc: 0.8419
Epoch 29/40
5376/6680 [=======================>{\ldots}] - ETA: 0s - loss: 0.0813 - acc: 0.9767
Epoch 00029: val\_loss did not improve
6680/6680 [==============================] - 0s 22us/step - loss: 0.0798 - acc: 0.9769 - val\_loss: 0.5705 - val\_acc: 0.8299
Epoch 30/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.0702 - acc: 0.9809
Epoch 00030: val\_loss improved from 0.52995 to 0.51938, saving model to saved\_models/weights.best.Resnet50.hdf5
6680/6680 [==============================] - 0s 17us/step - loss: 0.0717 - acc: 0.9793 - val\_loss: 0.5194 - val\_acc: 0.8431
Epoch 31/40
3840/6680 [================>{\ldots}] - ETA: 0s - loss: 0.0602 - acc: 0.9833
Epoch 00031: val\_loss did not improve
6680/6680 [==============================] - 0s 15us/step - loss: 0.0611 - acc: 0.9823 - val\_loss: 0.5384 - val\_acc: 0.8407
Epoch 32/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.0786 - acc: 0.9755
Epoch 00032: val\_loss did not improve
6680/6680 [==============================] - 0s 13us/step - loss: 0.0761 - acc: 0.9760 - val\_loss: 0.5826 - val\_acc: 0.8395
Epoch 33/40
4096/6680 [=================>{\ldots}] - ETA: 0s - loss: 0.0685 - acc: 0.9790
Epoch 00033: val\_loss did not improve
6680/6680 [==============================] - 0s 15us/step - loss: 0.0685 - acc: 0.9789 - val\_loss: 0.5473 - val\_acc: 0.8503
Epoch 34/40
4608/6680 [===================>{\ldots}] - ETA: 0s - loss: 0.0750 - acc: 0.9787
Epoch 00034: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.0768 - acc: 0.9768 - val\_loss: 0.5666 - val\_acc: 0.8359
Epoch 35/40
4352/6680 [==================>{\ldots}] - ETA: 0s - loss: 0.0683 - acc: 0.9818
Epoch 00035: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.0640 - acc: 0.9829 - val\_loss: 0.5680 - val\_acc: 0.8287
Epoch 36/40
4096/6680 [=================>{\ldots}] - ETA: 0s - loss: 0.0652 - acc: 0.9807
Epoch 00036: val\_loss did not improve
6680/6680 [==============================] - 0s 14us/step - loss: 0.0636 - acc: 0.9814 - val\_loss: 0.5690 - val\_acc: 0.8311
Epoch 37/40
3840/6680 [================>{\ldots}] - ETA: 0s - loss: 0.0600 - acc: 0.9823
Epoch 00037: val\_loss did not improve
6680/6680 [==============================] - 0s 15us/step - loss: 0.0581 - acc: 0.9823 - val\_loss: 0.5650 - val\_acc: 0.8275
Epoch 38/40
6400/6680 [===========================>..] - ETA: 0s - loss: 0.0759 - acc: 0.9775
Epoch 00038: val\_loss did not improve
6680/6680 [==============================] - 0s 20us/step - loss: 0.0757 - acc: 0.9775 - val\_loss: 0.5975 - val\_acc: 0.8335
Epoch 39/40
3840/6680 [================>{\ldots}] - ETA: 0s - loss: 0.0632 - acc: 0.9810
Epoch 00039: val\_loss did not improve
6680/6680 [==============================] - 0s 15us/step - loss: 0.0669 - acc: 0.9793 - val\_loss: 0.5424 - val\_acc: 0.8503
Epoch 40/40
6400/6680 [===========================>..] - ETA: 0s - loss: 0.0675 - acc: 0.9816
Epoch 00040: val\_loss did not improve
6680/6680 [==============================] - 0s 19us/step - loss: 0.0671 - acc: 0.9814 - val\_loss: 0.5758 - val\_acc: 0.8383

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}94}]:} <keras.callbacks.History at 0x7fa8953b45f8>
\end{Verbatim}
            
    \subsubsection{(IMPLEMENTATION) Load the Model with the Best Validation
Loss}\label{implementation-load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Load the model weights with the best validation loss.}
         \PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.Resnet50.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Test the
Model}\label{implementation-test-the-model}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 60\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Calculate classification accuracy on the test dataset.}
         \PY{n}{Resnet50\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Resnet50\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}Resnet50}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{Resnet50\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Resnet50\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Resnet50 Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Resnet50 Test accuracy: 83.3732\%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}112}]:} \PY{k}{def} \PY{n+nf}{evaluate\PYZus{}model}\PY{p}{(}\PY{n}{model\PYZus{}name}\PY{p}{)}\PY{p}{:}
              \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/Dog}\PY{l+s+si}{\PYZob{}model\PYZus{}name\PYZcb{}}\PY{l+s+s1}{Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{train} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
              \PY{n}{valid} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
              \PY{n}{test} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
              
              \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
              
              \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.}\PY{l+s+si}{\PYZob{}model\PYZus{}name\PYZcb{}}\PY{l+s+s1}{.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                         \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                        \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                        \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.}\PY{l+s+si}{\PYZob{}model\PYZus{}name\PYZcb{}}\PY{l+s+s1}{.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              
              \PY{n}{predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test}\PY{p}{]}
              \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}model\PYZus{}name\PYZcb{}}\PY{l+s+s1}{ Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \subsection{Test InceptionV3}\label{test-inceptionv3}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}113}]:} \PY{n}{evaluate\PYZus{}model}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{InceptionV3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5715 - acc: 0.4518
Epoch 00001: val\_loss improved from inf to 0.67998, saving model to saved\_models/weights.best.InceptionV3.hdf5
6680/6680 [==============================] - 4s 553us/step - loss: 2.5654 - acc: 0.4527 - val\_loss: 0.6800 - val\_acc: 0.7952
Epoch 2/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7598
Epoch 00002: val\_loss improved from 0.67998 to 0.54929, saving model to saved\_models/weights.best.InceptionV3.hdf5
6680/6680 [==============================] - 2s 338us/step - loss: 0.8212 - acc: 0.7602 - val\_loss: 0.5493 - val\_acc: 0.8275
Epoch 3/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.8179
Epoch 00003: val\_loss improved from 0.54929 to 0.54551, saving model to saved\_models/weights.best.InceptionV3.hdf5
6680/6680 [==============================] - 2s 319us/step - loss: 0.5891 - acc: 0.8184 - val\_loss: 0.5455 - val\_acc: 0.8371
Epoch 4/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8392
Epoch 00004: val\_loss improved from 0.54551 to 0.53323, saving model to saved\_models/weights.best.InceptionV3.hdf5
6680/6680 [==============================] - 2s 339us/step - loss: 0.5075 - acc: 0.8392 - val\_loss: 0.5332 - val\_acc: 0.8371
Epoch 5/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8591
Epoch 00005: val\_loss did not improve
6680/6680 [==============================] - 2s 319us/step - loss: 0.4373 - acc: 0.8588 - val\_loss: 0.5621 - val\_acc: 0.8299
Epoch 6/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8696
Epoch 00006: val\_loss did not improve
6680/6680 [==============================] - 2s 328us/step - loss: 0.3997 - acc: 0.8689 - val\_loss: 0.5380 - val\_acc: 0.8467
Epoch 7/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8762
Epoch 00007: val\_loss improved from 0.53323 to 0.52130, saving model to saved\_models/weights.best.InceptionV3.hdf5
6680/6680 [==============================] - 2s 338us/step - loss: 0.3822 - acc: 0.8765 - val\_loss: 0.5213 - val\_acc: 0.8551
Epoch 8/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8915
Epoch 00008: val\_loss improved from 0.52130 to 0.50271, saving model to saved\_models/weights.best.InceptionV3.hdf5
6680/6680 [==============================] - 2s 323us/step - loss: 0.3210 - acc: 0.8918 - val\_loss: 0.5027 - val\_acc: 0.8515
Epoch 9/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9087
Epoch 00009: val\_loss did not improve
6680/6680 [==============================] - 2s 324us/step - loss: 0.2790 - acc: 0.9088 - val\_loss: 0.5270 - val\_acc: 0.8419
Epoch 10/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9156
Epoch 00010: val\_loss did not improve
6680/6680 [==============================] - 2s 311us/step - loss: 0.2540 - acc: 0.9157 - val\_loss: 0.5341 - val\_acc: 0.8431
Epoch 11/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9138
Epoch 00011: val\_loss did not improve
6680/6680 [==============================] - 2s 326us/step - loss: 0.2502 - acc: 0.9138 - val\_loss: 0.5473 - val\_acc: 0.8491
Epoch 12/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9255
Epoch 00012: val\_loss did not improve
6680/6680 [==============================] - 2s 336us/step - loss: 0.2197 - acc: 0.9253 - val\_loss: 0.5555 - val\_acc: 0.8443
Epoch 13/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9291
Epoch 00013: val\_loss did not improve
6680/6680 [==============================] - 2s 317us/step - loss: 0.2092 - acc: 0.9290 - val\_loss: 0.5570 - val\_acc: 0.8551
Epoch 14/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9286
Epoch 00014: val\_loss did not improve
6680/6680 [==============================] - 2s 323us/step - loss: 0.2133 - acc: 0.9284 - val\_loss: 0.5371 - val\_acc: 0.8479
Epoch 15/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9386
Epoch 00015: val\_loss did not improve
6680/6680 [==============================] - 2s 336us/step - loss: 0.1905 - acc: 0.9383 - val\_loss: 0.5166 - val\_acc: 0.8479
Epoch 16/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9404
Epoch 00016: val\_loss did not improve
6680/6680 [==============================] - 2s 325us/step - loss: 0.1805 - acc: 0.9403 - val\_loss: 0.5454 - val\_acc: 0.8551
Epoch 17/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9395
Epoch 00017: val\_loss did not improve
6680/6680 [==============================] - 2s 327us/step - loss: 0.1753 - acc: 0.9392 - val\_loss: 0.5607 - val\_acc: 0.8467
Epoch 18/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9443
Epoch 00018: val\_loss did not improve
6680/6680 [==============================] - 2s 321us/step - loss: 0.1600 - acc: 0.9443 - val\_loss: 0.5504 - val\_acc: 0.8491
Epoch 19/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9504
Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 2s 334us/step - loss: 0.1407 - acc: 0.9504 - val\_loss: 0.5338 - val\_acc: 0.8599
Epoch 20/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9515
Epoch 00020: val\_loss did not improve
6680/6680 [==============================] - 2s 322us/step - loss: 0.1360 - acc: 0.9515 - val\_loss: 0.5414 - val\_acc: 0.8575
Epoch 21/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9561
Epoch 00021: val\_loss did not improve
6680/6680 [==============================] - 2s 341us/step - loss: 0.1298 - acc: 0.9560 - val\_loss: 0.5886 - val\_acc: 0.8527
Epoch 22/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9539
Epoch 00022: val\_loss did not improve
6680/6680 [==============================] - 2s 336us/step - loss: 0.1328 - acc: 0.9537 - val\_loss: 0.5592 - val\_acc: 0.8587
Epoch 23/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9620
Epoch 00023: val\_loss did not improve
6680/6680 [==============================] - 2s 328us/step - loss: 0.1175 - acc: 0.9617 - val\_loss: 0.5540 - val\_acc: 0.8611
Epoch 24/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9557
Epoch 00024: val\_loss did not improve
6680/6680 [==============================] - 2s 322us/step - loss: 0.1284 - acc: 0.9555 - val\_loss: 0.5769 - val\_acc: 0.8479
Epoch 25/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9636
Epoch 00025: val\_loss did not improve
6680/6680 [==============================] - 2s 323us/step - loss: 0.1093 - acc: 0.9636 - val\_loss: 0.5824 - val\_acc: 0.8527
Epoch 26/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9627
Epoch 00026: val\_loss did not improve
6680/6680 [==============================] - 2s 342us/step - loss: 0.1076 - acc: 0.9629 - val\_loss: 0.6106 - val\_acc: 0.8539
Epoch 27/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9686
Epoch 00027: val\_loss did not improve
6680/6680 [==============================] - 2s 322us/step - loss: 0.0928 - acc: 0.9686 - val\_loss: 0.6074 - val\_acc: 0.8455
Epoch 28/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9716
Epoch 00028: val\_loss did not improve
6680/6680 [==============================] - 2s 332us/step - loss: 0.0959 - acc: 0.9716 - val\_loss: 0.6011 - val\_acc: 0.8503
Epoch 29/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9698
Epoch 00029: val\_loss did not improve
6680/6680 [==============================] - 2s 345us/step - loss: 0.0938 - acc: 0.9698 - val\_loss: 0.5930 - val\_acc: 0.8551
Epoch 30/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9647
Epoch 00030: val\_loss did not improve
6680/6680 [==============================] - 2s 330us/step - loss: 0.1023 - acc: 0.9647 - val\_loss: 0.6287 - val\_acc: 0.8395
Epoch 31/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9663
Epoch 00031: val\_loss did not improve
6680/6680 [==============================] - 2s 352us/step - loss: 0.1007 - acc: 0.9665 - val\_loss: 0.6119 - val\_acc: 0.8491
Epoch 32/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9727
Epoch 00032: val\_loss did not improve
6680/6680 [==============================] - 2s 316us/step - loss: 0.0878 - acc: 0.9726 - val\_loss: 0.6135 - val\_acc: 0.8503
Epoch 33/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9719
Epoch 00033: val\_loss did not improve
6680/6680 [==============================] - 2s 345us/step - loss: 0.0862 - acc: 0.9719 - val\_loss: 0.6531 - val\_acc: 0.8479
Epoch 34/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9725
Epoch 00034: val\_loss did not improve
6680/6680 [==============================] - 2s 346us/step - loss: 0.0803 - acc: 0.9726 - val\_loss: 0.6303 - val\_acc: 0.8551
Epoch 35/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9757
Epoch 00035: val\_loss did not improve
6680/6680 [==============================] - 2s 338us/step - loss: 0.0755 - acc: 0.9757 - val\_loss: 0.6103 - val\_acc: 0.8539
Epoch 36/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9803
Epoch 00036: val\_loss did not improve
6680/6680 [==============================] - 2s 335us/step - loss: 0.0647 - acc: 0.9802 - val\_loss: 0.6338 - val\_acc: 0.8491
Epoch 37/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9778
Epoch 00037: val\_loss did not improve
6680/6680 [==============================] - 2s 314us/step - loss: 0.0713 - acc: 0.9778 - val\_loss: 0.6243 - val\_acc: 0.8527
Epoch 38/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9769
Epoch 00038: val\_loss did not improve
6680/6680 [==============================] - 2s 326us/step - loss: 0.0662 - acc: 0.9768 - val\_loss: 0.6441 - val\_acc: 0.8455
Epoch 39/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9764
Epoch 00039: val\_loss did not improve
6680/6680 [==============================] - 2s 319us/step - loss: 0.0732 - acc: 0.9765 - val\_loss: 0.6413 - val\_acc: 0.8551
Epoch 40/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9785
Epoch 00040: val\_loss did not improve
6680/6680 [==============================] - 2s 313us/step - loss: 0.0654 - acc: 0.9786 - val\_loss: 0.6327 - val\_acc: 0.8479

InceptionV3 Test accuracy: 82.1770\%

    \end{Verbatim}

    \subsection{Test Xception}\label{test-xception}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}114}]:} \PY{n}{evaluate\PYZus{}model}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/40
6656/6680 [============================>.] - ETA: 0s - loss: 2.5827 - acc: 0.4737
Epoch 00001: val\_loss improved from inf to 0.76704, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 7s 1ms/step - loss: 2.5763 - acc: 0.4751 - val\_loss: 0.7670 - val\_acc: 0.7701
Epoch 2/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.7288 - acc: 0.7797
Epoch 00002: val\_loss improved from 0.76704 to 0.56306, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 3s 506us/step - loss: 0.7281 - acc: 0.7801 - val\_loss: 0.5631 - val\_acc: 0.8168
Epoch 3/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8388
Epoch 00003: val\_loss improved from 0.56306 to 0.51163, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 3s 504us/step - loss: 0.5234 - acc: 0.8389 - val\_loss: 0.5116 - val\_acc: 0.8395
Epoch 4/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8582
Epoch 00004: val\_loss did not improve
6680/6680 [==============================] - 3s 496us/step - loss: 0.4424 - acc: 0.8579 - val\_loss: 0.5128 - val\_acc: 0.8311
Epoch 5/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8714
Epoch 00005: val\_loss improved from 0.51163 to 0.48214, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 3s 484us/step - loss: 0.4028 - acc: 0.8713 - val\_loss: 0.4821 - val\_acc: 0.8479
Epoch 6/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8917
Epoch 00006: val\_loss did not improve
6680/6680 [==============================] - 3s 481us/step - loss: 0.3393 - acc: 0.8918 - val\_loss: 0.4876 - val\_acc: 0.8383
Epoch 7/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8969
Epoch 00007: val\_loss did not improve
6680/6680 [==============================] - 3s 490us/step - loss: 0.3233 - acc: 0.8969 - val\_loss: 0.4842 - val\_acc: 0.8491
Epoch 8/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9090
Epoch 00008: val\_loss improved from 0.48214 to 0.47469, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 3s 457us/step - loss: 0.2843 - acc: 0.9082 - val\_loss: 0.4747 - val\_acc: 0.8455
Epoch 9/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9073
Epoch 00009: val\_loss did not improve
6680/6680 [==============================] - 3s 496us/step - loss: 0.2681 - acc: 0.9076 - val\_loss: 0.4765 - val\_acc: 0.8515
Epoch 10/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9244
Epoch 00010: val\_loss improved from 0.47469 to 0.47380, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 3s 493us/step - loss: 0.2324 - acc: 0.9243 - val\_loss: 0.4738 - val\_acc: 0.8551
Epoch 11/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9343
Epoch 00011: val\_loss improved from 0.47380 to 0.46853, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 3s 481us/step - loss: 0.2111 - acc: 0.9343 - val\_loss: 0.4685 - val\_acc: 0.8575
Epoch 12/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9345
Epoch 00012: val\_loss did not improve
6680/6680 [==============================] - 3s 478us/step - loss: 0.1998 - acc: 0.9346 - val\_loss: 0.4718 - val\_acc: 0.8539
Epoch 13/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9396
Epoch 00013: val\_loss did not improve
6680/6680 [==============================] - 3s 491us/step - loss: 0.1791 - acc: 0.9397 - val\_loss: 0.4718 - val\_acc: 0.8491
Epoch 14/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9467
Epoch 00014: val\_loss did not improve
6680/6680 [==============================] - 3s 487us/step - loss: 0.1649 - acc: 0.9469 - val\_loss: 0.4914 - val\_acc: 0.8467
Epoch 15/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9536
Epoch 00015: val\_loss improved from 0.46853 to 0.46763, saving model to saved\_models/weights.best.Xception.hdf5
6680/6680 [==============================] - 3s 488us/step - loss: 0.1490 - acc: 0.9537 - val\_loss: 0.4676 - val\_acc: 0.8503
Epoch 16/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9542
Epoch 00016: val\_loss did not improve
6680/6680 [==============================] - 3s 478us/step - loss: 0.1405 - acc: 0.9540 - val\_loss: 0.4849 - val\_acc: 0.8491
Epoch 17/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9554
Epoch 00017: val\_loss did not improve
6680/6680 [==============================] - 3s 492us/step - loss: 0.1397 - acc: 0.9555 - val\_loss: 0.4686 - val\_acc: 0.8587
Epoch 18/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9641
Epoch 00018: val\_loss did not improve
6680/6680 [==============================] - 3s 493us/step - loss: 0.1177 - acc: 0.9639 - val\_loss: 0.4739 - val\_acc: 0.8527
Epoch 19/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9534
Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 3s 475us/step - loss: 0.1527 - acc: 0.9536 - val\_loss: 0.4697 - val\_acc: 0.8515
Epoch 20/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9648
Epoch 00020: val\_loss did not improve
6680/6680 [==============================] - 3s 499us/step - loss: 0.1131 - acc: 0.9647 - val\_loss: 0.4815 - val\_acc: 0.8467
Epoch 21/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9669
Epoch 00021: val\_loss did not improve
6680/6680 [==============================] - 3s 475us/step - loss: 0.1132 - acc: 0.9671 - val\_loss: 0.4761 - val\_acc: 0.8575
Epoch 22/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9703
Epoch 00022: val\_loss did not improve
6680/6680 [==============================] - 3s 453us/step - loss: 0.0990 - acc: 0.9702 - val\_loss: 0.4784 - val\_acc: 0.8467
Epoch 23/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9706
Epoch 00023: val\_loss did not improve
6680/6680 [==============================] - 3s 518us/step - loss: 0.0990 - acc: 0.9705 - val\_loss: 0.4899 - val\_acc: 0.8575
Epoch 24/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9718
Epoch 00024: val\_loss did not improve
6680/6680 [==============================] - 3s 472us/step - loss: 0.0929 - acc: 0.9717 - val\_loss: 0.5125 - val\_acc: 0.8479
Epoch 25/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9722
Epoch 00025: val\_loss did not improve
6680/6680 [==============================] - 3s 475us/step - loss: 0.0915 - acc: 0.9722 - val\_loss: 0.5028 - val\_acc: 0.8527
Epoch 26/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9713
Epoch 00026: val\_loss did not improve
6680/6680 [==============================] - 3s 486us/step - loss: 0.0943 - acc: 0.9713 - val\_loss: 0.5109 - val\_acc: 0.8443
Epoch 27/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9719
Epoch 00027: val\_loss did not improve
6680/6680 [==============================] - 3s 462us/step - loss: 0.0867 - acc: 0.9720 - val\_loss: 0.5110 - val\_acc: 0.8575
Epoch 28/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9760
Epoch 00028: val\_loss did not improve
6680/6680 [==============================] - 3s 478us/step - loss: 0.0763 - acc: 0.9760 - val\_loss: 0.5136 - val\_acc: 0.8587
Epoch 29/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9785
Epoch 00029: val\_loss did not improve
6680/6680 [==============================] - 3s 506us/step - loss: 0.0730 - acc: 0.9786 - val\_loss: 0.4878 - val\_acc: 0.8551
Epoch 30/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9742
Epoch 00030: val\_loss did not improve
6680/6680 [==============================] - 3s 468us/step - loss: 0.0785 - acc: 0.9740 - val\_loss: 0.5077 - val\_acc: 0.8515
Epoch 31/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9761
Epoch 00031: val\_loss did not improve
6680/6680 [==============================] - 3s 489us/step - loss: 0.0834 - acc: 0.9760 - val\_loss: 0.5120 - val\_acc: 0.8599
Epoch 32/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9787
Epoch 00032: val\_loss did not improve
6680/6680 [==============================] - 3s 491us/step - loss: 0.0719 - acc: 0.9786 - val\_loss: 0.5171 - val\_acc: 0.8515
Epoch 33/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9775
Epoch 00033: val\_loss did not improve
6680/6680 [==============================] - 3s 471us/step - loss: 0.0750 - acc: 0.9774 - val\_loss: 0.5412 - val\_acc: 0.8503
Epoch 34/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9769
Epoch 00034: val\_loss did not improve
6680/6680 [==============================] - 3s 493us/step - loss: 0.0748 - acc: 0.9768 - val\_loss: 0.5121 - val\_acc: 0.8479
Epoch 35/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9815
Epoch 00035: val\_loss did not improve
6680/6680 [==============================] - 3s 489us/step - loss: 0.0608 - acc: 0.9816 - val\_loss: 0.5161 - val\_acc: 0.8539
Epoch 36/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9835
Epoch 00036: val\_loss did not improve
6680/6680 [==============================] - 3s 491us/step - loss: 0.0572 - acc: 0.9832 - val\_loss: 0.5167 - val\_acc: 0.8575
Epoch 37/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9796
Epoch 00037: val\_loss did not improve
6680/6680 [==============================] - 3s 485us/step - loss: 0.0708 - acc: 0.9796 - val\_loss: 0.5068 - val\_acc: 0.8551
Epoch 38/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9830
Epoch 00038: val\_loss did not improve
6680/6680 [==============================] - 3s 456us/step - loss: 0.0566 - acc: 0.9831 - val\_loss: 0.5268 - val\_acc: 0.8503
Epoch 39/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9847
Epoch 00039: val\_loss did not improve
6680/6680 [==============================] - 3s 499us/step - loss: 0.0564 - acc: 0.9847 - val\_loss: 0.5412 - val\_acc: 0.8587
Epoch 40/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9865
Epoch 00040: val\_loss did not improve
6680/6680 [==============================] - 3s 491us/step - loss: 0.0478 - acc: 0.9865 - val\_loss: 0.5140 - val\_acc: 0.8527

Xception Test accuracy: 86.4833\%

    \end{Verbatim}

    \subsection{Test VGG19}\label{test-vgg19}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{n}{evaluate\PYZus{}model}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{VGG19}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 11.2693 - acc: 0.0667
Epoch 00001: val\_loss improved from inf to 3.72060, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 2s 289us/step - loss: 10.9012 - acc: 0.0711 - val\_loss: 3.7206 - val\_acc: 0.2611
Epoch 2/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 4.3036 - acc: 0.1823
Epoch 00002: val\_loss improved from 3.72060 to 3.32851, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 1s 76us/step - loss: 4.2450 - acc: 0.1850 - val\_loss: 3.3285 - val\_acc: 0.3305
Epoch 3/40
6656/6680 [============================>.] - ETA: 0s - loss: 3.0732 - acc: 0.3030
Epoch 00003: val\_loss improved from 3.32851 to 2.08408, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 71us/step - loss: 3.0707 - acc: 0.3033 - val\_loss: 2.0841 - val\_acc: 0.5222
Epoch 4/40
6400/6680 [===========================>..] - ETA: 0s - loss: 2.4116 - acc: 0.4152
Epoch 00004: val\_loss improved from 2.08408 to 1.54634, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 66us/step - loss: 2.4017 - acc: 0.4171 - val\_loss: 1.5463 - val\_acc: 0.5976
Epoch 5/40
6656/6680 [============================>.] - ETA: 0s - loss: 1.9984 - acc: 0.4862
Epoch 00005: val\_loss improved from 1.54634 to 1.28888, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 71us/step - loss: 1.9970 - acc: 0.4862 - val\_loss: 1.2889 - val\_acc: 0.6647
Epoch 6/40
5632/6680 [========================>{\ldots}] - ETA: 0s - loss: 1.7553 - acc: 0.5334
Epoch 00006: val\_loss improved from 1.28888 to 1.12088, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 68us/step - loss: 1.7363 - acc: 0.5367 - val\_loss: 1.1209 - val\_acc: 0.6838
Epoch 7/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 1.5147 - acc: 0.5752
Epoch 00007: val\_loss improved from 1.12088 to 1.03119, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 64us/step - loss: 1.5110 - acc: 0.5768 - val\_loss: 1.0312 - val\_acc: 0.7126
Epoch 8/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 1.3837 - acc: 0.6178
Epoch 00008: val\_loss improved from 1.03119 to 0.99243, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 72us/step - loss: 1.3901 - acc: 0.6160 - val\_loss: 0.9924 - val\_acc: 0.7066
Epoch 9/40
5632/6680 [========================>{\ldots}] - ETA: 0s - loss: 1.2829 - acc: 0.6445
Epoch 00009: val\_loss improved from 0.99243 to 0.90862, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 69us/step - loss: 1.2811 - acc: 0.6452 - val\_loss: 0.9086 - val\_acc: 0.7293
Epoch 10/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 1.1980 - acc: 0.6591
Epoch 00010: val\_loss improved from 0.90862 to 0.87498, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 66us/step - loss: 1.1909 - acc: 0.6632 - val\_loss: 0.8750 - val\_acc: 0.7437
Epoch 11/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 1.1108 - acc: 0.6774
Epoch 00011: val\_loss improved from 0.87498 to 0.84602, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 63us/step - loss: 1.1109 - acc: 0.6778 - val\_loss: 0.8460 - val\_acc: 0.7629
Epoch 12/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 1.0303 - acc: 0.6943
Epoch 00012: val\_loss improved from 0.84602 to 0.80580, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 73us/step - loss: 1.0307 - acc: 0.6975 - val\_loss: 0.8058 - val\_acc: 0.7545
Epoch 13/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.9854 - acc: 0.7025
Epoch 00013: val\_loss improved from 0.80580 to 0.78999, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 61us/step - loss: 0.9748 - acc: 0.7063 - val\_loss: 0.7900 - val\_acc: 0.7569
Epoch 14/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.9132 - acc: 0.7235
Epoch 00014: val\_loss improved from 0.78999 to 0.77613, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 61us/step - loss: 0.9193 - acc: 0.7220 - val\_loss: 0.7761 - val\_acc: 0.7677
Epoch 15/40
5632/6680 [========================>{\ldots}] - ETA: 0s - loss: 0.8846 - acc: 0.7377
Epoch 00015: val\_loss improved from 0.77613 to 0.75923, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 72us/step - loss: 0.8854 - acc: 0.7364 - val\_loss: 0.7592 - val\_acc: 0.7689
Epoch 16/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.8463 - acc: 0.7378
Epoch 00016: val\_loss improved from 0.75923 to 0.75640, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 62us/step - loss: 0.8319 - acc: 0.7416 - val\_loss: 0.7564 - val\_acc: 0.7677
Epoch 17/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.8252 - acc: 0.7480
Epoch 00017: val\_loss improved from 0.75640 to 0.74942, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 60us/step - loss: 0.8240 - acc: 0.7497 - val\_loss: 0.7494 - val\_acc: 0.7629
Epoch 18/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.7411 - acc: 0.7679
Epoch 00018: val\_loss improved from 0.74942 to 0.70955, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 71us/step - loss: 0.7410 - acc: 0.7680 - val\_loss: 0.7096 - val\_acc: 0.7820
Epoch 19/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.7301 - acc: 0.7797
Epoch 00019: val\_loss did not improve
6680/6680 [==============================] - 0s 66us/step - loss: 0.7432 - acc: 0.7737 - val\_loss: 0.7260 - val\_acc: 0.7749
Epoch 20/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.7243 - acc: 0.7744
Epoch 00020: val\_loss improved from 0.70955 to 0.70844, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 66us/step - loss: 0.7239 - acc: 0.7753 - val\_loss: 0.7084 - val\_acc: 0.7772
Epoch 21/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.7014 - acc: 0.7836
Epoch 00021: val\_loss improved from 0.70844 to 0.70069, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 73us/step - loss: 0.6989 - acc: 0.7855 - val\_loss: 0.7007 - val\_acc: 0.7737
Epoch 22/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.6404 - acc: 0.8020
Epoch 00022: val\_loss did not improve
6680/6680 [==============================] - 0s 65us/step - loss: 0.6404 - acc: 0.8019 - val\_loss: 0.7060 - val\_acc: 0.7928
Epoch 23/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.6241 - acc: 0.8039
Epoch 00023: val\_loss improved from 0.70069 to 0.69016, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 64us/step - loss: 0.6242 - acc: 0.8009 - val\_loss: 0.6902 - val\_acc: 0.7964
Epoch 24/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.5910 - acc: 0.8120
Epoch 00024: val\_loss improved from 0.69016 to 0.68485, saving model to saved\_models/weights.best.VGG19.hdf5
6680/6680 [==============================] - 0s 71us/step - loss: 0.5970 - acc: 0.8103 - val\_loss: 0.6848 - val\_acc: 0.7940
Epoch 25/40
5632/6680 [========================>{\ldots}] - ETA: 0s - loss: 0.6148 - acc: 0.8033
Epoch 00025: val\_loss did not improve
6680/6680 [==============================] - 0s 65us/step - loss: 0.5952 - acc: 0.8075 - val\_loss: 0.7178 - val\_acc: 0.7844
Epoch 26/40
5632/6680 [========================>{\ldots}] - ETA: 0s - loss: 0.5721 - acc: 0.8168
Epoch 00026: val\_loss did not improve
6680/6680 [==============================] - 0s 66us/step - loss: 0.5672 - acc: 0.8187 - val\_loss: 0.7215 - val\_acc: 0.7796
Epoch 27/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.5315 - acc: 0.8288
Epoch 00027: val\_loss did not improve
6680/6680 [==============================] - 0s 65us/step - loss: 0.5372 - acc: 0.8268 - val\_loss: 0.6868 - val\_acc: 0.7772
Epoch 28/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.5882 - acc: 0.8130
Epoch 00028: val\_loss did not improve
6680/6680 [==============================] - 0s 73us/step - loss: 0.5879 - acc: 0.8111 - val\_loss: 0.6925 - val\_acc: 0.7868
Epoch 29/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.5423 - acc: 0.8270
Epoch 00029: val\_loss did not improve
6680/6680 [==============================] - 0s 60us/step - loss: 0.5376 - acc: 0.8284 - val\_loss: 0.6849 - val\_acc: 0.7880
Epoch 30/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.5277 - acc: 0.8324
Epoch 00030: val\_loss did not improve
6680/6680 [==============================] - 0s 59us/step - loss: 0.5290 - acc: 0.8322 - val\_loss: 0.6968 - val\_acc: 0.7976
Epoch 31/40
6656/6680 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.8302
Epoch 00031: val\_loss did not improve
6680/6680 [==============================] - 0s 69us/step - loss: 0.5280 - acc: 0.8307 - val\_loss: 0.6941 - val\_acc: 0.7976
Epoch 32/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.5130 - acc: 0.8325
Epoch 00032: val\_loss did not improve
6680/6680 [==============================] - 0s 62us/step - loss: 0.5114 - acc: 0.8331 - val\_loss: 0.6907 - val\_acc: 0.7856
Epoch 33/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.4476 - acc: 0.8563
Epoch 00033: val\_loss did not improve
6680/6680 [==============================] - 0s 64us/step - loss: 0.4471 - acc: 0.8563 - val\_loss: 0.6967 - val\_acc: 0.7844
Epoch 34/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.4521 - acc: 0.8538
Epoch 00034: val\_loss did not improve
6680/6680 [==============================] - 0s 65us/step - loss: 0.4527 - acc: 0.8536 - val\_loss: 0.6849 - val\_acc: 0.7988
Epoch 35/40
5632/6680 [========================>{\ldots}] - ETA: 0s - loss: 0.4282 - acc: 0.8594
Epoch 00035: val\_loss did not improve
6680/6680 [==============================] - 0s 70us/step - loss: 0.4357 - acc: 0.8558 - val\_loss: 0.7063 - val\_acc: 0.7928
Epoch 36/40
5632/6680 [========================>{\ldots}] - ETA: 0s - loss: 0.4337 - acc: 0.8580
Epoch 00036: val\_loss did not improve
6680/6680 [==============================] - 0s 68us/step - loss: 0.4344 - acc: 0.8573 - val\_loss: 0.6994 - val\_acc: 0.7964
Epoch 37/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.4368 - acc: 0.8584
Epoch 00037: val\_loss did not improve
6680/6680 [==============================] - 1s 75us/step - loss: 0.4362 - acc: 0.8587 - val\_loss: 0.7329 - val\_acc: 0.7844
Epoch 38/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.4149 - acc: 0.8651
Epoch 00038: val\_loss did not improve
6680/6680 [==============================] - 0s 66us/step - loss: 0.4201 - acc: 0.8630 - val\_loss: 0.7485 - val\_acc: 0.7856
Epoch 39/40
5888/6680 [=========================>{\ldots}] - ETA: 0s - loss: 0.3972 - acc: 0.8660
Epoch 00039: val\_loss did not improve
6680/6680 [==============================] - 0s 63us/step - loss: 0.3986 - acc: 0.8665 - val\_loss: 0.7137 - val\_acc: 0.7844
Epoch 40/40
6144/6680 [==========================>{\ldots}] - ETA: 0s - loss: 0.4141 - acc: 0.8665
Epoch 00040: val\_loss did not improve
6680/6680 [==============================] - 0s 61us/step - loss: 0.4138 - acc: 0.8651 - val\_loss: 0.6935 - val\_acc: 0.7928

VGG19 Test accuracy: 77.3923\%

    \end{Verbatim}

    \subsection{Test LightGBM}\label{test-lightgbm}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{k+kn}{from} \PY{n+nn}{lightgbm} \PY{k}{import} \PY{n}{LGBMClassifier}
          \PY{n}{lgbm} \PY{o}{=} \PY{n}{LGBMClassifier}\PY{p}{(}\PY{n}{objective}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{multiclass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{lgbm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}Resnet50}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{train\PYZus{}Resnet50}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{train\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                  \PY{n}{eval\PYZus{}set}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}Resnet50}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{valid\PYZus{}Resnet50}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{valid\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[1]	valid\_0's multi\_logloss: 2.93992
[2]	valid\_0's multi\_logloss: 2.76174
[3]	valid\_0's multi\_logloss: 2.66124
[4]	valid\_0's multi\_logloss: 2.5714
[5]	valid\_0's multi\_logloss: 2.49337
[6]	valid\_0's multi\_logloss: 2.42221
[7]	valid\_0's multi\_logloss: 2.36437
[8]	valid\_0's multi\_logloss: 2.30971
[9]	valid\_0's multi\_logloss: 2.25785
[10]	valid\_0's multi\_logloss: 2.21151
[11]	valid\_0's multi\_logloss: 2.16577
[12]	valid\_0's multi\_logloss: 2.12489
[13]	valid\_0's multi\_logloss: 2.0899
[14]	valid\_0's multi\_logloss: 2.05487
[15]	valid\_0's multi\_logloss: 2.02101
[16]	valid\_0's multi\_logloss: 1.98921
[17]	valid\_0's multi\_logloss: 1.9581
[18]	valid\_0's multi\_logloss: 1.92964
[19]	valid\_0's multi\_logloss: 1.90176
[20]	valid\_0's multi\_logloss: 1.87688
[21]	valid\_0's multi\_logloss: 1.8503
[22]	valid\_0's multi\_logloss: 1.82647
[23]	valid\_0's multi\_logloss: 1.80308
[24]	valid\_0's multi\_logloss: 1.78055
[25]	valid\_0's multi\_logloss: 1.75879
[26]	valid\_0's multi\_logloss: 1.7369
[27]	valid\_0's multi\_logloss: 1.71522
[28]	valid\_0's multi\_logloss: 1.69388
[29]	valid\_0's multi\_logloss: 1.67296
[30]	valid\_0's multi\_logloss: 1.65246
[31]	valid\_0's multi\_logloss: 1.63437
[32]	valid\_0's multi\_logloss: 1.61799
[33]	valid\_0's multi\_logloss: 1.59937
[34]	valid\_0's multi\_logloss: 1.58228
[35]	valid\_0's multi\_logloss: 1.56495
[36]	valid\_0's multi\_logloss: 1.54784
[37]	valid\_0's multi\_logloss: 1.5321
[38]	valid\_0's multi\_logloss: 1.51681
[39]	valid\_0's multi\_logloss: 1.50213
[40]	valid\_0's multi\_logloss: 1.48398
[41]	valid\_0's multi\_logloss: 1.46967
[42]	valid\_0's multi\_logloss: 1.45707
[43]	valid\_0's multi\_logloss: 1.44233
[44]	valid\_0's multi\_logloss: 1.42942
[45]	valid\_0's multi\_logloss: 1.41579
[46]	valid\_0's multi\_logloss: 1.40486
[47]	valid\_0's multi\_logloss: 1.39258
[48]	valid\_0's multi\_logloss: 1.38039
[49]	valid\_0's multi\_logloss: 1.36892
[50]	valid\_0's multi\_logloss: 1.35807
[51]	valid\_0's multi\_logloss: 1.34697
[52]	valid\_0's multi\_logloss: 1.33513
[53]	valid\_0's multi\_logloss: 1.32485
[54]	valid\_0's multi\_logloss: 1.31462
[55]	valid\_0's multi\_logloss: 1.30541
[56]	valid\_0's multi\_logloss: 1.29726
[57]	valid\_0's multi\_logloss: 1.28844
[58]	valid\_0's multi\_logloss: 1.27971
[59]	valid\_0's multi\_logloss: 1.27257
[60]	valid\_0's multi\_logloss: 1.26519
[61]	valid\_0's multi\_logloss: 1.25793
[62]	valid\_0's multi\_logloss: 1.24948
[63]	valid\_0's multi\_logloss: 1.24174
[64]	valid\_0's multi\_logloss: 1.23414
[65]	valid\_0's multi\_logloss: 1.22663
[66]	valid\_0's multi\_logloss: 1.21862
[67]	valid\_0's multi\_logloss: 1.21193
[68]	valid\_0's multi\_logloss: 1.20495
[69]	valid\_0's multi\_logloss: 1.19754
[70]	valid\_0's multi\_logloss: 1.19189
[71]	valid\_0's multi\_logloss: 1.18555
[72]	valid\_0's multi\_logloss: 1.17954
[73]	valid\_0's multi\_logloss: 1.17398
[74]	valid\_0's multi\_logloss: 1.16976
[75]	valid\_0's multi\_logloss: 1.16649
[76]	valid\_0's multi\_logloss: 1.16208
[77]	valid\_0's multi\_logloss: 1.15719
[78]	valid\_0's multi\_logloss: 1.15398
[79]	valid\_0's multi\_logloss: 1.149
[80]	valid\_0's multi\_logloss: 1.14488
[81]	valid\_0's multi\_logloss: 1.14163
[82]	valid\_0's multi\_logloss: 1.13747
[83]	valid\_0's multi\_logloss: 1.13329
[84]	valid\_0's multi\_logloss: 1.12905
[85]	valid\_0's multi\_logloss: 1.12466
[86]	valid\_0's multi\_logloss: 1.12023
[87]	valid\_0's multi\_logloss: 1.1169
[88]	valid\_0's multi\_logloss: 1.113
[89]	valid\_0's multi\_logloss: 1.10931
[90]	valid\_0's multi\_logloss: 1.10395
[91]	valid\_0's multi\_logloss: 1.10149
[92]	valid\_0's multi\_logloss: 1.09764
[93]	valid\_0's multi\_logloss: 1.09399
[94]	valid\_0's multi\_logloss: 1.08955
[95]	valid\_0's multi\_logloss: 1.08722
[96]	valid\_0's multi\_logloss: 1.08275
[97]	valid\_0's multi\_logloss: 1.07833
[98]	valid\_0's multi\_logloss: 1.0739
[99]	valid\_0's multi\_logloss: 1.06959
[100]	valid\_0's multi\_logloss: 1.06506

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}105}]:} LGBMClassifier(boosting\_type='gbdt', class\_weight=None, colsample\_bytree=1.0,
                  learning\_rate=0.1, max\_depth=-1, min\_child\_samples=20,
                  min\_child\_weight=0.001, min\_split\_gain=0.0, n\_estimators=100,
                  n\_jobs=-1, num\_leaves=31, objective='multiclass',
                  random\_state=None, reg\_alpha=0.0, reg\_lambda=0.0, silent=True,
                  subsample=1.0, subsample\_for\_bin=200000, subsample\_freq=1)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}110}]:} \PY{n}{lgbm\PYZus{}predictions} \PY{o}{=} \PY{n}{lgbm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}Resnet50}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{test\PYZus{}Resnet50}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
          \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{lgbm\PYZus{}predictions}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{lgbm\PYZus{}predictions}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LightGBM Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
LightGBM Test accuracy: 72.9665\%

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Predict Dog Breed with the
Model}\label{implementation-predict-dog-breed-with-the-model}

Write a function that takes an image path as input and returns the dog
breed (\texttt{Affenpinscher}, \texttt{Afghan\_hound}, etc) that is
predicted by your model.

Similar to the analogous function in Step 5, your function should have
three steps: 1. Extract the bottleneck features corresponding to the
chosen CNN model. 2. Supply the bottleneck features as input to the
model to return the predicted vector. Note that the argmax of this
prediction vector gives the index of the predicted dog breed. 3. Use the
\texttt{dog\_names} array defined in Step 0 of this notebook to return
the corresponding breed.

The functions to extract the bottleneck features can be found in
\texttt{extract\_bottleneck\_features.py}, and they have been imported
in an earlier code cell. To obtain the bottleneck features corresponding
to your chosen CNN architecture, you need to use the function

\begin{verbatim}
extract_{network}
\end{verbatim}

where \texttt{\{network\}}, in the above filename, should be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}155}]:} \PY{k+kn}{import} \PY{n+nn}{cv2}                
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}                        
          \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline    
          
          \PY{k}{def} \PY{n+nf}{display\PYZus{}image}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{k}{if} \PY{n}{url}\PY{p}{:}
                  \PY{n}{req} \PY{o}{=} \PY{n}{urllib}\PY{o}{.}\PY{n}{request}\PY{o}{.}\PY{n}{urlopen}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
                  \PY{n}{arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{asarray}\PY{p}{(}\PY{n+nb}{bytearray}\PY{p}{(}\PY{n}{req}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imdecode}\PY{p}{(}\PY{n}{arr}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
              \PY{n}{cv\PYZus{}rgb} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2RGB}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cv\PYZus{}rgb}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Write a function that takes a path to an image as input}
          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} and returns the dog breed that is predicted by the model.}
          \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{n}{extract\PYZus{}InceptionV3}
          \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{n}{extract\PYZus{}Resnet50}
          \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{n}{extract\PYZus{}VGG16}
          \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{n}{extract\PYZus{}VGG19}
          \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{n}{extract\PYZus{}Xception}
          
          \PY{n}{extract\PYZus{}bottleneck\PYZus{}features} \PY{o}{=} \PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{InceptionV3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{extract\PYZus{}InceptionV3}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Resnet50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{extract\PYZus{}Resnet50}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{VGG16}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{extract\PYZus{}VGG16}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{VGG19}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{extract\PYZus{}VGG19}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{extract\PYZus{}Xception}
          \PY{p}{\PYZcb{}}
          
          \PY{k}{def} \PY{n+nf}{predict\PYZus{}image}\PY{p}{(}\PY{n}{model\PYZus{}name}\PY{p}{,} \PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}    
              \PY{n}{img} \PY{o}{=} \PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
              \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{extract\PYZus{}bottleneck\PYZus{}features}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]}\PY{p}{(}\PY{n}{img}\PY{p}{)}
              
              \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{bottleneck\PYZus{}features}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.}\PY{l+s+si}{\PYZob{}model\PYZus{}name\PYZcb{}}\PY{l+s+s1}{.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              
              \PY{k}{return} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}features}\PY{p}{)}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}120}]:} \PY{n}{sns}\PY{o}{.}\PY{n}{reset\PYZus{}orig}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{k+kn}{import} \PY{n+nn}{random}
         \PY{n}{file} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{file}\PY{p}{)}
         \PY{n}{display\PYZus{}image}\PY{p}{(}\PY{n}{file}\PY{p}{)}
         \PY{n}{predict\PYZus{}image}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{file}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
dogImages/test/050.Chinese\_shar-pei/Chinese\_shar-pei\_03556.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_90_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}82}]:} 'Chinese\_shar-pei'
\end{Verbatim}
            
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 6: Write your Algorithm

Write an algorithm that accepts a file path to an image and first
determines whether the image contains a human, dog, or neither. Then, -
if a \textbf{dog} is detected in the image, return the predicted breed.
- if a \textbf{human} is detected in the image, return the resembling
dog breed. - if \textbf{neither} is detected in the image, provide
output that indicates an error.

You are welcome to write your own functions for detecting humans and
dogs in images, but feel free to use the \texttt{face\_detector} and
\texttt{dog\_detector} functions developed above. You are
\textbf{required} to use your CNN from Step 5 to predict dog breed.

Some sample output for our algorithm is provided below, but feel free to
design your own user experience!

\begin{figure}
\centering
\includegraphics{images/sample_human_output.png}
\caption{Sample Human Output}
\end{figure}

\subsubsection{(IMPLEMENTATION) Write your
Algorithm}\label{implementation-write-your-algorithm}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}157}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} DONE: Write your algorithm.}
          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
          \PY{k}{def} \PY{n+nf}{detect\PYZus{}image}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{model\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{k}{if} \PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{It}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{s a }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{predict\PYZus{}image(model\PYZus{}name, img\PYZus{}path)\PYZcb{}!}\PY{l+s+s2}{\PYZdq{}}
              \PY{k}{elif} \PY{n}{dnn\PYZus{}face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{l+m+mf}{0.99}\PY{p}{,} \PY{n}{url}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{You look like a }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{predict\PYZus{}image(model\PYZus{}name, img\PYZus{}path)\PYZcb{}!}\PY{l+s+s2}{\PYZdq{}}
              \PY{k}{else}\PY{p}{:}
                  \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error: the image could not be recognized}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}134}]:} \PY{k+kn}{import} \PY{n+nn}{random}
          \PY{n}{file} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)} \PY{c+c1}{\PYZsh{} or human\PYZus{}files}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{file}\PY{p}{)}
          \PY{n}{display\PYZus{}image}\PY{p}{(}\PY{n}{file}\PY{p}{)}
          \PY{n}{detect\PYZus{}image}\PY{p}{(}\PY{n}{file}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
lfw/Lord\_Hutton/Lord\_Hutton\_0002.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_93_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}134}]:} 'You look like a Cavalier\_king\_charles\_spaniel!'
\end{Verbatim}
            
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 7: Test Your Algorithm

In this section, you will take your new algorithm for a spin! What kind
of dog does the algorithm think that \textbf{you} look like? If you have
a dog, does it predict your dog's breed accurately? If you have a cat,
does it mistakenly think that your cat is a dog?

\subsubsection{(IMPLEMENTATION) Test Your Algorithm on Sample
Images!}\label{implementation-test-your-algorithm-on-sample-images}

Test your algorithm at least six images on your computer. Feel free to
use any images you like. Use at least two human and two dog images.

\textbf{Question 6:} Is the output better than you expected :) ? Or
worse :( ? Provide at least three possible points of improvement for
your algorithm.

\textbf{Answer:} The algorithm was able to match nearly all the dog
images with their breed, except in cases where the images were not part
of any of the 133 classes. As for the human face matching, it is hard to
know what resembling dog breed to expect, so the output was very
diverse. In order to improve the algorithm, data augmentation is always
a good method for reducing overfitting. After this, hyperparameter
tuning, such as weight initialization method can helping for the
optimization. Furthermore, in order to improve the optimization of the
neural network, more techniques for adjusting the learning rate can be
useful, such as cyclical learning rates (1).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  2015 - Leslie N. Smith. Cyclical Learning Rates for Training Neural
  Networks
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}158}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} DONE: Execute your algorithm from Step 6 on}
          \PY{c+c1}{\PYZsh{}\PYZsh{} at least 6 images on your computer.}
          \PY{c+c1}{\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
          \PY{k}{def} \PY{n+nf}{run\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{model\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{url}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
              \PY{n}{display\PYZus{}image}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{url}\PY{p}{)}
              \PY{k}{return} \PY{n}{detect\PYZus{}image}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{model\PYZus{}name}\PY{p}{,} \PY{n}{url}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}166}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lfw/Michael\PYZus{}Jackson/Michael\PYZus{}Jackson\PYZus{}0006.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
lfw/Michael\_Jackson/Michael\_Jackson\_0006.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_96_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}166}]:} 'You look like a Dachshund!'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}167}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/chihuahua.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/chihuahua.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_97_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}167}]:} "It's a Chihuahua!"
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}172}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/muffin.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/muffin.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_98_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}172}]:} "It's a Chinese\_crested!"
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}173}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/labradoodle.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/labradoodle.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_99_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}173}]:} "It's a Kuvasz!"
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}174}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/snapchat.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/snapchat.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_100_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}174}]:} 'You look like a Doberman\_pinscher!'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}182}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/beagle.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/beagle.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_101_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}182}]:} "It's a Beagle!"
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}184}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/shibe.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/shibe.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_102_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}184}]:} "It's a Akita!"
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}191}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/arnold.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/arnold.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_103_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}191}]:} 'You look like a Chinese\_crested!'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}175}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/cat.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/cat.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_104_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}175}]:} "It's a Maltese!"
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}176}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/doge.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/doge.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_105_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}176}]:} 'Error: the image could not be recognized'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}192}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/cat2.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/cat2.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_106_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}192}]:} 'Error: the image could not be recognized'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}179}]:} \PY{n}{run\PYZus{}detector}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}samples/german\PYZus{}shepherd.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
test\_samples/german\_shepherd.jpg

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_107_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}179}]:} "It's a Canaan\_dog!"
\end{Verbatim}
            

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
